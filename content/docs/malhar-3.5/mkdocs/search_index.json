{
    "docs": [
        {
            "location": "/", 
            "text": "Apache Apex Malhar\n\n\nApache Apex Malhar is an open source operator and codec library that can be used with the \nApache Apex\n platform to build real-time streaming applications.  Enabling users to extract value quickly, Malhar operators help get data in, analyze it in real-time, and get data out of Hadoop.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.\n\n\n\n\nCapabilities common across Malhar operators\n\n\nFor most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of Apex. Hence, they have following core streaming runtime capabilities\n\n\n\n\nFault tolerance\n \u2013 Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.\n\n\nProcessing guarantees\n \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once, and at-most once WITHOUT requiring the user to write any additional code.  Some operators, like MQTT operator, deal with source systems that can not track processed data and hence need the operators to keep track of the data.  Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this.  Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.\n\n\nDynamic updates\n \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.\n\n\nEase of extensibility\n \u2013 Malhar operators are based on templates that are easy to extend.\n\n\nPartitioning support\n \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system.  For example with Kafka, the operator can automatically scale up or down based on the changes in the number of Kafka partitions.\n\n\n\n\nOperator Library Overview\n\n\nInput/output connectors\n\n\nBelow is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator\n\n\n\n\nFile Systems\n \u2013 Most streaming analytics use cases require the data to be stored in HDFS or perhaps S3 if the application is running in AWS.  Users often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share.  Apex supports input \n output operators for HDFS, S3, NFS \n Local Files.  There are also File Splitter and Block Reader operators, which can accelecate processing of large files by splitting and paralellizing the work across non-overlapping sets of file blocks.\n\n\nRelational Databases\n \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. Apex supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL, Sqlite, etc.\n\n\nNoSQL Databases\n \u2013 NoSQL key-value pair databases like Cassandra \n HBase are a common part of streaming analytics application architectures to lookup reference data or store results.  Malhar has operators for HBase, Cassandra, Accumulo, Aerospike, MongoDB, and CouchDB.\n\n\nMessaging Systems\n \u2013 Kafka, JMS, and similar systems are the workhorses of messaging infrastructure in most enterprises.  Malhar has a robust, industry-tested set of operators to read and write Kafka, JMS, ZeroMQ, and RabbitMQ messages.\n\n\nNotification Systems\n \u2013 Malhar includes an operator for sending notifications via SMTP.\n\n\nIn-memory Databases \n Caching platforms\n - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached and Redis.\n\n\nSocial Media\n - Malhar includes an operator to connect to the popular Twitter stream fire hose.\n\n\nProtocols\n - Malhar provides connectors that can communicate in HTTP, RSS, Socket, WebSocket, FTP, and MQTT.\n\n\n\n\nParsers\n\n\nThere are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM \n SAX), JSON (flat map converter), Apache log files, syslog, etc.\n\n\nStream manipulation\n\n\nStreaming data inevitably needs processing to clean, filter, tag, summarize, etc. The goal of Malhar is to enable the application developer to focus on WHAT needs to be done to the stream to get it in the right format and not worry about the HOW.  Malhar has several operators to perform the common stream manipulation actions like \u2013 GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.\n\n\nCompute\n\n\nOne of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant, stateful, etc.  Malhar takes this responsibility away from the application developer by providing a variety of out of the box computational operators.\n\n\nBelow is just a snapshot of the compute operators available in Malhar\n\n\n\n\nStatistics and math - Various mathematical and statistical computations over application defined time windows.\n\n\nFiltering and pattern matching\n\n\nSorting, maps, frequency, TopN, BottomN\n\n\nRandom data generators\n\n\n\n\nLanguages Support\n\n\nMigrating to a new platform often requires re-use of the existing code that would be difficult or time-consuming to re-write.  With this in mind, Malhar supports invocation of code written in other languages by wrapping them in one of the library operators, and allows execution of software written in:\n\n\n\n\nJavaScript\n\n\nPython\n\n\nR\n\n\nRuby", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/#apache-apex-malhar", 
            "text": "Apache Apex Malhar is an open source operator and codec library that can be used with the  Apache Apex  platform to build real-time streaming applications.  Enabling users to extract value quickly, Malhar operators help get data in, analyze it in real-time, and get data out of Hadoop.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/#capabilities-common-across-malhar-operators", 
            "text": "For most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of Apex. Hence, they have following core streaming runtime capabilities   Fault tolerance  \u2013 Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.  Processing guarantees  \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once, and at-most once WITHOUT requiring the user to write any additional code.  Some operators, like MQTT operator, deal with source systems that can not track processed data and hence need the operators to keep track of the data.  Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this.  Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.  Dynamic updates  \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.  Ease of extensibility  \u2013 Malhar operators are based on templates that are easy to extend.  Partitioning support  \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system.  For example with Kafka, the operator can automatically scale up or down based on the changes in the number of Kafka partitions.", 
            "title": "Capabilities common across Malhar operators"
        }, 
        {
            "location": "/#operator-library-overview", 
            "text": "", 
            "title": "Operator Library Overview"
        }, 
        {
            "location": "/#inputoutput-connectors", 
            "text": "Below is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator   File Systems  \u2013 Most streaming analytics use cases require the data to be stored in HDFS or perhaps S3 if the application is running in AWS.  Users often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share.  Apex supports input   output operators for HDFS, S3, NFS   Local Files.  There are also File Splitter and Block Reader operators, which can accelecate processing of large files by splitting and paralellizing the work across non-overlapping sets of file blocks.  Relational Databases  \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. Apex supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL, Sqlite, etc.  NoSQL Databases  \u2013 NoSQL key-value pair databases like Cassandra   HBase are a common part of streaming analytics application architectures to lookup reference data or store results.  Malhar has operators for HBase, Cassandra, Accumulo, Aerospike, MongoDB, and CouchDB.  Messaging Systems  \u2013 Kafka, JMS, and similar systems are the workhorses of messaging infrastructure in most enterprises.  Malhar has a robust, industry-tested set of operators to read and write Kafka, JMS, ZeroMQ, and RabbitMQ messages.  Notification Systems  \u2013 Malhar includes an operator for sending notifications via SMTP.  In-memory Databases   Caching platforms  - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached and Redis.  Social Media  - Malhar includes an operator to connect to the popular Twitter stream fire hose.  Protocols  - Malhar provides connectors that can communicate in HTTP, RSS, Socket, WebSocket, FTP, and MQTT.", 
            "title": "Input/output connectors"
        }, 
        {
            "location": "/#parsers", 
            "text": "There are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM   SAX), JSON (flat map converter), Apache log files, syslog, etc.", 
            "title": "Parsers"
        }, 
        {
            "location": "/#stream-manipulation", 
            "text": "Streaming data inevitably needs processing to clean, filter, tag, summarize, etc. The goal of Malhar is to enable the application developer to focus on WHAT needs to be done to the stream to get it in the right format and not worry about the HOW.  Malhar has several operators to perform the common stream manipulation actions like \u2013 GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.", 
            "title": "Stream manipulation"
        }, 
        {
            "location": "/#compute", 
            "text": "One of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant, stateful, etc.  Malhar takes this responsibility away from the application developer by providing a variety of out of the box computational operators.  Below is just a snapshot of the compute operators available in Malhar   Statistics and math - Various mathematical and statistical computations over application defined time windows.  Filtering and pattern matching  Sorting, maps, frequency, TopN, BottomN  Random data generators", 
            "title": "Compute"
        }, 
        {
            "location": "/#languages-support", 
            "text": "Migrating to a new platform often requires re-use of the existing code that would be difficult or time-consuming to re-write.  With this in mind, Malhar supports invocation of code written in other languages by wrapping them in one of the library operators, and allows execution of software written in:   JavaScript  Python  R  Ruby", 
            "title": "Languages Support"
        }, 
        {
            "location": "/operators/kafkaInputOperator/", 
            "text": "KAFKA INPUT OPERATOR\n\n\nIntroduction: About Kafka Input Operator\n\n\nThis is an input operator that consumes data from Kafka messaging system for further processing in Apex. Kafka Input Operator is an fault-tolerant and scalable Malhar Operator.\n\n\nWhy is it needed ?\n\n\nKafka is a pull-based and distributed publish subscribe messaging system, topics are partitioned and replicated across\nnodes. Kafka input operator is needed when you want to read data from multiple\npartitions of a Kafka topic in parallel in an Apex application.\n\n\nAbstractKafkaInputOperator\n\n\nThis is the abstract implementation that serves as base class for consuming messages from Kafka messaging system. This class doesn\u2019t have any ports.\n\n\n\n\nConfiguration Parameters\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\nmaxTuplesPerWindow\n\n\nControls the maximum number of messages emitted in each streaming window from this operator. Minimum value is 1. Default value = MAX_VALUE \n\n\n\n\n\n\nidempotentStorageManager\n\n\nThis is an instance of IdempotentStorageManager. Idempotency ensures that the operator will process the same set of messages in a window before and after a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same messages again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Default Value = com.datatorrent.lib.io.IdempotentStorageManager.\nNoopIdempotentStorageManager\n\n\n\n\n\n\nstrategy\n\n\nOperator supports two types of partitioning strategies, ONE_TO_ONE and ONE_TO_MANY.\n\n\nONE_TO_ONE: If this is enabled, the AppMaster creates one input operator instance per Kafka topic partition. So the number of Kafka topic partitions equals the number of operator instances.\n\n\nONE_TO_MANY: The AppMaster creates K = min(initialPartitionCount, N) Kafka input operator instances where N is the number of Kafka topic partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Kafka topic partition. For example, if initialPartitionCount = 5 and number of Kafka partitions(N) = 2 then AppMaster creates 2 Kafka input operator instances.\nDefault Value = ONE_TO_ONE\n\n\n\n\n\n\nmsgRateUpperBound\n\n\nMaximum messages upper bound. Operator repartitions when the \nmsgProcessedPS\n exceeds this bound. \nmsgProcessedPS\n is the average number of messages processed per second by this operator.\n\n\n\n\n\n\nbyteRateUpperBound\n\n\nMaximum bytes upper bound. Operator repartitions when the \nbytesPS\n exceeds this bound. \nbytesPS\n is the average number of bytes processed per second by this operator.\n\n\n\n\n\n\n\n\noffsetManager\n\n\nThis is an optional parameter that is useful when the application restarts or start at specific offsets (offsets are explained below)\n\n\n\n\n\n\nrepartitionInterval\n\n\nInterval specified in milliseconds. This value specifies the minimum time required between two repartition actions. Default Value = 30 Seconds\n\n\n\n\n\n\nrepartitionCheckInterval\n\n\nInterval specified in milliseconds. This value specifies the minimum interval between two offset updates. Default Value = 5 Seconds\n\n\n\n\n\n\ninitialPartitionCount\n\n\nWhen the ONE_TO_MANY partition strategy is enabled, this value indicates the number of Kafka input operator instances. Default Value = 1\n\n\n\n\n\n\nconsumer\n\n\nThis is an instance of com.datatorrent.contrib.kafka.KafkaConsumer. Default Value = Instance of SimpleKafkaConsumer.\n\n\n\n\n\n\n\n\nAbstract Methods\n\n\nvoid emitTuple(Message message): Abstract method that emits tuples\nextracted from Kafka message.\n\n\nKafkaConsumer\n\n\nThis is an abstract implementation of Kafka consumer. It sends the fetch\nrequests to the leading brokers of Kafka partitions. For each request,\nit receives the set of messages and stores them into the buffer which is\nArrayBlockingQueue. SimpleKafkaConsumer\u00a0which extends\nKafkaConsumer and serves the functionality of Simple Consumer API and\nHighLevelKafkaConsumer which extends KafkaConsumer and \u00a0serves the\nfunctionality of High Level Consumer API.\n\n\nPre-requisites\n\n\nThis operator referred the Kafka Consumer API of version\n0.8.1.1. So, this operator will work with any 0.8.x and 0.7.x version of Apache Kafka.\n\n\nConfiguration Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\nzookeeper\n\n\nString\n\n\n\n\nSpecifies the zookeeper quorum of Kafka clusters that you want to consume messages from. zookeeper \u00a0is a string in the form of hostname1:port1,hostname2:port2,hostname3:port3 \u00a0where hostname1,hostname2,hostname3 are hosts and port1,port2,port3 are ports of zookeeper server. \u00a0If the topic name is the same across the Kafka clusters and want to consume data from these clusters, then configure the zookeeper as follows: c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6\n\n\nwhere\n\n\nc1,c2,c3 indicates the cluster names, hs1,hs2,hs3,hs4,hs5,hs6 are zookeeper hosts and p1,p2,p3,p4,p5,p6 are corresponding ports. Here, cluster name is optional in case of single cluster\n\n\n\n\n\n\ncacheSize\n\n\nint\n\n\n1024\n\n\nMaximum of buffered messages hold in memory.\n\n\n\n\n\n\ntopic\n\n\nString\n\n\ndefault_topic\n\n\nIndicates the name of the topic.\n\n\n\n\n\n\ninitialOffset\n\n\nString\n\n\nlatest\n\n\nIndicates the type of offset i.e, \u201cearliest or latest\u201d. If initialOffset is \u201clatest\u201d, then the operator consumes messages from latest point of Kafka queue. If initialOffset is \u201cearliest\u201d, then the operator consumes messages starting from message queue. This can be overridden by OffsetManager.\n\n\n\n\n\n\n\n\n\nAbstract Methods\n\n\n\n\nvoid commitOffset(): Commit the offsets at checkpoint.\n\n\nMap \nKafkaPartition, Long\n getCurrentOffsets(): Return the current\n    offset status.\n\n\nresetPartitionsAndOffset(Set \nKafkaPartition\n partitionIds,\n    Map \nKafkaPartition, Long\n startOffset): Reset the partitions with\n    parittionIds and offsets with startOffset.\n\n\n\n\nConfiguration Parameters\u00a0for SimpleKafkaConsumer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\nbufferSize\n\n\nint\n\n\n1 MB\n\n\nSpecifies the maximum total size of messages for each fetch request.\n\n\n\n\n\n\nmetadataRefreshInterval\n\n\nint\n\n\n30 Seconds\n\n\nInterval in between refresh the metadata change(broker change) in milliseconds. Enabling metadata refresh guarantees an automatic reconnect when a new broker is elected as the host. A value of -1 disables this feature.\n\n\n\n\n\n\nmetadataRefreshRetryLimit\n\n\nint\n\n\n-1\n\n\nSpecifies the maximum brokers' metadata refresh retry limit. -1 means unlimited retry.\n\n\n\n\n\n\n\n\n\nOffsetManager\n\n\nThis is an interface for offset management and is useful when consuming data\nfrom specified offsets. Updates the offsets for all the Kafka partitions\nperiodically. Below is the code snippet:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\npublic interface OffsetManager\n{\n  public Map\nKafkaPartition, Long\n loadInitialOffsets();\n  public void updateOffsets(Map\nKafkaPartition, Long\n offsetsOfPartitions);\n}\n\n\n\n\nAbstract Methods\n\n\nMap \nKafkaPartition, Long\n loadInitialOffsets(): Specifies the initial offset for consuming messages; called at the activation stage.\n\n\nupdateOffsets(Map \nKafkaPartition, Long\n offsetsOfPartitions): \u00a0This\nmethod is called at every repartitionCheckInterval to update offsets.\n\n\nPartitioning\n\n\nThe logical instance of the KafkaInputOperator acts as the Partitioner\nas well as a StatsListener. This is because the\nAbstractKafkaInputOperator implements both the\ncom.datatorrent.api.Partitioner and com.datatorrent.api.StatsListener\ninterfaces and provides an implementation of definePartitions(...) and\nprocessStats(...) which makes it auto-scalable.\n\n\nResponse processStats(BatchedOperatorStats stats)\n\n\nThe application master invokes this method on the logical instance with\nthe stats (tuplesProcessedPS, bytesPS, etc.) of each partition.\nRe-partitioning happens based on whether any new Kafka partitions added for\nthe topic or bytesPS and msgPS cross their respective upper bounds.\n\n\nDefinePartitions\n\n\nBased on the repartitionRequired field of the Response object which is\nreturned by processStats(...) method, the application master invokes\ndefinePartitions(...) on the logical instance which is also the\npartitioner instance. Dynamic partition can be disabled by setting the\nparameter repartitionInterval value to a negative value.\n\n\nAbstractSinglePortKafkaInputOperator\n\n\nThis class extends AbstractKafkaInputOperator and having single output\nport, will emit the messages through this port.\n\n\nPorts\n\n\noutputPort \nT\n: Tuples extracted from Kafka messages are emitted through\nthis port.\n\n\nAbstract Methods\n\n\nT getTuple(Message msg) : Converts the Kafka message to tuple.\n\n\nConcrete Classes\n\n\n\n\n\n\nKafkaSinglePortStringInputOperator :\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts string from Kafka message.\n\n\n\n\n\n\nKafkaSinglePortByteArrayInputOperator:\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts byte array from Kafka message.\n\n\n\n\n\n\nApplication Example\n\n\nThis section builds an Apex application using Kafka input operator.\nBelow is the code snippet:\n\n\n@ApplicationAnnotation(name = \nKafkaApp\n)\npublic class ExampleKafkaApplication implements StreamingApplication\n{\n@Override\npublic void populateDAG(DAG dag, Configuration entries)\n{\n  KafkaSinglePortByteArrayInputOperator input =  dag.addOperator(\nMessageReader\n, new KafkaSinglePortByteArrayInputOperator());\n\n  ConsoleOutputOperator output = dag.addOperator(\nOutput\n, new ConsoleOutputOperator());\n\n  dag.addStream(\nMessageData\n, input.outputPort, output.input);\n}\n}\n\n\n\n\nBelow is the configuration for \u201ctest\u201d Kafka topic name and\n\u201clocalhost:2181\u201d is the zookeeper forum:\n\n\nproperty\n\n\nname\ndt.operator.MessageReader.prop.topic\n/name\n\n\nvalue\ntest\n/value\n\n\n/property\n\n\n\nproperty\n\n\nname\ndt.operator.KafkaInputOperator.prop.zookeeper\n/nam\n\n\nvalue\nlocalhost:2181\n/value\n\n\n/property", 
            "title": "Kafka Input"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#kafka-input-operator", 
            "text": "", 
            "title": "KAFKA INPUT OPERATOR"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#introduction-about-kafka-input-operator", 
            "text": "This is an input operator that consumes data from Kafka messaging system for further processing in Apex. Kafka Input Operator is an fault-tolerant and scalable Malhar Operator.", 
            "title": "Introduction: About Kafka Input Operator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#why-is-it-needed", 
            "text": "Kafka is a pull-based and distributed publish subscribe messaging system, topics are partitioned and replicated across\nnodes. Kafka input operator is needed when you want to read data from multiple\npartitions of a Kafka topic in parallel in an Apex application.", 
            "title": "Why is it needed ?"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstractkafkainputoperator", 
            "text": "This is the abstract implementation that serves as base class for consuming messages from Kafka messaging system. This class doesn\u2019t have any ports.", 
            "title": "AbstractKafkaInputOperator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters", 
            "text": "Parameter  Description    maxTuplesPerWindow  Controls the maximum number of messages emitted in each streaming window from this operator. Minimum value is 1. Default value = MAX_VALUE     idempotentStorageManager  This is an instance of IdempotentStorageManager. Idempotency ensures that the operator will process the same set of messages in a window before and after a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same messages again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Default Value = com.datatorrent.lib.io.IdempotentStorageManager. NoopIdempotentStorageManager    strategy  Operator supports two types of partitioning strategies, ONE_TO_ONE and ONE_TO_MANY.  ONE_TO_ONE: If this is enabled, the AppMaster creates one input operator instance per Kafka topic partition. So the number of Kafka topic partitions equals the number of operator instances.  ONE_TO_MANY: The AppMaster creates K = min(initialPartitionCount, N) Kafka input operator instances where N is the number of Kafka topic partitions. If K is less than N, the remaining topic partitions are assigned to the K operator instances in round-robin fashion. If K is less than initialPartitionCount, the AppMaster creates one input operator instance per Kafka topic partition. For example, if initialPartitionCount = 5 and number of Kafka partitions(N) = 2 then AppMaster creates 2 Kafka input operator instances.\nDefault Value = ONE_TO_ONE    msgRateUpperBound  Maximum messages upper bound. Operator repartitions when the  msgProcessedPS  exceeds this bound.  msgProcessedPS  is the average number of messages processed per second by this operator.    byteRateUpperBound  Maximum bytes upper bound. Operator repartitions when the  bytesPS  exceeds this bound.  bytesPS  is the average number of bytes processed per second by this operator.     offsetManager  This is an optional parameter that is useful when the application restarts or start at specific offsets (offsets are explained below)    repartitionInterval  Interval specified in milliseconds. This value specifies the minimum time required between two repartition actions. Default Value = 30 Seconds    repartitionCheckInterval  Interval specified in milliseconds. This value specifies the minimum interval between two offset updates. Default Value = 5 Seconds    initialPartitionCount  When the ONE_TO_MANY partition strategy is enabled, this value indicates the number of Kafka input operator instances. Default Value = 1    consumer  This is an instance of com.datatorrent.contrib.kafka.KafkaConsumer. Default Value = Instance of SimpleKafkaConsumer.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods", 
            "text": "void emitTuple(Message message): Abstract method that emits tuples\nextracted from Kafka message.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#kafkaconsumer", 
            "text": "This is an abstract implementation of Kafka consumer. It sends the fetch\nrequests to the leading brokers of Kafka partitions. For each request,\nit receives the set of messages and stores them into the buffer which is\nArrayBlockingQueue. SimpleKafkaConsumer\u00a0which extends\nKafkaConsumer and serves the functionality of Simple Consumer API and\nHighLevelKafkaConsumer which extends KafkaConsumer and \u00a0serves the\nfunctionality of High Level Consumer API.", 
            "title": "KafkaConsumer"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#pre-requisites", 
            "text": "This operator referred the Kafka Consumer API of version\n0.8.1.1. So, this operator will work with any 0.8.x and 0.7.x version of Apache Kafka.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters_1", 
            "text": "Parameter  Type  Default  Description    zookeeper  String   Specifies the zookeeper quorum of Kafka clusters that you want to consume messages from. zookeeper \u00a0is a string in the form of hostname1:port1,hostname2:port2,hostname3:port3 \u00a0where hostname1,hostname2,hostname3 are hosts and port1,port2,port3 are ports of zookeeper server. \u00a0If the topic name is the same across the Kafka clusters and want to consume data from these clusters, then configure the zookeeper as follows: c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6  where  c1,c2,c3 indicates the cluster names, hs1,hs2,hs3,hs4,hs5,hs6 are zookeeper hosts and p1,p2,p3,p4,p5,p6 are corresponding ports. Here, cluster name is optional in case of single cluster    cacheSize  int  1024  Maximum of buffered messages hold in memory.    topic  String  default_topic  Indicates the name of the topic.    initialOffset  String  latest  Indicates the type of offset i.e, \u201cearliest or latest\u201d. If initialOffset is \u201clatest\u201d, then the operator consumes messages from latest point of Kafka queue. If initialOffset is \u201cearliest\u201d, then the operator consumes messages starting from message queue. This can be overridden by OffsetManager.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_1", 
            "text": "void commitOffset(): Commit the offsets at checkpoint.  Map  KafkaPartition, Long  getCurrentOffsets(): Return the current\n    offset status.  resetPartitionsAndOffset(Set  KafkaPartition  partitionIds,\n    Map  KafkaPartition, Long  startOffset): Reset the partitions with\n    parittionIds and offsets with startOffset.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#configuration-parameters-for-simplekafkaconsumer", 
            "text": "Parameter  Type  Default  Description    bufferSize  int  1 MB  Specifies the maximum total size of messages for each fetch request.    metadataRefreshInterval  int  30 Seconds  Interval in between refresh the metadata change(broker change) in milliseconds. Enabling metadata refresh guarantees an automatic reconnect when a new broker is elected as the host. A value of -1 disables this feature.    metadataRefreshRetryLimit  int  -1  Specifies the maximum brokers' metadata refresh retry limit. -1 means unlimited retry.", 
            "title": "Configuration Parameters\u00a0for SimpleKafkaConsumer"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#offsetmanager", 
            "text": "This is an interface for offset management and is useful when consuming data\nfrom specified offsets. Updates the offsets for all the Kafka partitions\nperiodically. Below is the code snippet:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  public interface OffsetManager\n{\n  public Map KafkaPartition, Long  loadInitialOffsets();\n  public void updateOffsets(Map KafkaPartition, Long  offsetsOfPartitions);\n}", 
            "title": "OffsetManager"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_2", 
            "text": "Map  KafkaPartition, Long  loadInitialOffsets(): Specifies the initial offset for consuming messages; called at the activation stage.  updateOffsets(Map  KafkaPartition, Long  offsetsOfPartitions): \u00a0This\nmethod is called at every repartitionCheckInterval to update offsets.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#partitioning", 
            "text": "The logical instance of the KafkaInputOperator acts as the Partitioner\nas well as a StatsListener. This is because the\nAbstractKafkaInputOperator implements both the\ncom.datatorrent.api.Partitioner and com.datatorrent.api.StatsListener\ninterfaces and provides an implementation of definePartitions(...) and\nprocessStats(...) which makes it auto-scalable.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#response-processstatsbatchedoperatorstats-stats", 
            "text": "The application master invokes this method on the logical instance with\nthe stats (tuplesProcessedPS, bytesPS, etc.) of each partition.\nRe-partitioning happens based on whether any new Kafka partitions added for\nthe topic or bytesPS and msgPS cross their respective upper bounds.", 
            "title": "Response processStats(BatchedOperatorStats stats)"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#definepartitions", 
            "text": "Based on the repartitionRequired field of the Response object which is\nreturned by processStats(...) method, the application master invokes\ndefinePartitions(...) on the logical instance which is also the\npartitioner instance. Dynamic partition can be disabled by setting the\nparameter repartitionInterval value to a negative value.", 
            "title": "DefinePartitions"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstractsingleportkafkainputoperator", 
            "text": "This class extends AbstractKafkaInputOperator and having single output\nport, will emit the messages through this port.", 
            "title": "AbstractSinglePortKafkaInputOperator"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#ports", 
            "text": "outputPort  T : Tuples extracted from Kafka messages are emitted through\nthis port.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#abstract-methods_3", 
            "text": "T getTuple(Message msg) : Converts the Kafka message to tuple.", 
            "title": "Abstract Methods"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#concrete-classes", 
            "text": "KafkaSinglePortStringInputOperator :\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts string from Kafka message.    KafkaSinglePortByteArrayInputOperator:\nThis class extends AbstractSinglePortKafkaInputOperator and getTuple() method extracts byte array from Kafka message.", 
            "title": "Concrete Classes"
        }, 
        {
            "location": "/operators/kafkaInputOperator/#application-example", 
            "text": "This section builds an Apex application using Kafka input operator.\nBelow is the code snippet:  @ApplicationAnnotation(name =  KafkaApp )\npublic class ExampleKafkaApplication implements StreamingApplication\n{\n@Override\npublic void populateDAG(DAG dag, Configuration entries)\n{\n  KafkaSinglePortByteArrayInputOperator input =  dag.addOperator( MessageReader , new KafkaSinglePortByteArrayInputOperator());\n\n  ConsoleOutputOperator output = dag.addOperator( Output , new ConsoleOutputOperator());\n\n  dag.addStream( MessageData , input.outputPort, output.input);\n}\n}  Below is the configuration for \u201ctest\u201d Kafka topic name and\n\u201clocalhost:2181\u201d is the zookeeper forum:  property  name dt.operator.MessageReader.prop.topic /name  value test /value  /property  property  name dt.operator.KafkaInputOperator.prop.zookeeper /nam  value localhost:2181 /value  /property", 
            "title": "Application Example"
        }, 
        {
            "location": "/operators/file_splitter/", 
            "text": "File Splitter\n\n\nThis is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits. \n\n\nWhy is it needed?\n\n\nIt is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.\n\n\nClass Diagram\n\n\n\n\nAbstractFileSplitter\n\n\nThe abstract implementation defines the logic of processing \nFileInfo\n. This comprises the following tasks -  \n\n\n\n\n\n\nbuilding \nFileMetadata\n per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.\n\n\n\n\n\n\ncreating \nBlockMetadataIterator\n from \nFileMetadata\n. The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.\n\n\n\n\n\n\nretrieving \nBlockMetadata.FileBlockMetadata\n from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by \nblocksThreshold\n setting which by default is 1.  \n\n\n\n\n\n\nThe main utility method that performs all the above tasks is the \nprocess()\n method. Concrete implementations can invoke this method whenever they have data to process.\n\n\nPorts\n\n\nDeclares only output ports on which file metadata and block metadata are emitted.\n\n\n\n\nfilesMetadataOutput: metadata for each file is emitted on this port. \n\n\nblocksMetadataOutput: metadata for each block is emitted on this port. \n\n\n\n\nprocess()\n method\n\n\nWhen process() is invoked, any pending blocks from the current file are emitted on the 'blocksMetadataOutput' port. If the threshold for blocks per window is still not met then a new input file is processed - corresponding metadata is emitted on 'filesMetadataOutput' and more of its blocks are emitted. This operation is repeated until the \nblocksThreshold\n is reached or there are no more new files.\n\n\n  protected void process()\n  {\n    if (blockMetadataIterator != null \n blockCount \n blocksThreshold) {\n      emitBlockMetadata();\n    }\n\n    FileInfo fileInfo;\n    while (blockCount \n blocksThreshold \n (fileInfo = getFileInfo()) != null) {\n      if (!processFileInfo(fileInfo)) {\n        break;\n      }\n    }\n  }\n\n\n\n\nAbstract methods\n\n\n\n\n\n\nFileInfo getFileInfo()\n: called from within the \nprocess()\n and provides the next file to process.\n\n\n\n\n\n\nlong getDefaultBlockSize()\n: provides the block size which is used when user hasn't configured the size.\n\n\n\n\n\n\nFileStatus getFileStatus(Path path)\n: provides the \norg.apache.hadoop.fs.FileStatus\n instance for a path.   \n\n\n\n\n\n\nConfiguration\n\n\n\n\nblockSize\n: size of a block.\n\n\nblocksThreshold\n: threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.\n\n\n\n\nFileSplitterBase\n\n\nSimple operator that receives tuples of type \nFileInfo\n on its \ninput\n port. \nFileInfo\n contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.\n\n\n\nThe upstream operator emits tuples of type \nFileInfo\n on its output port which is connected to splitter input port. The downstream receives tuples of type \nBlockMetadata.FileBlockMetadata\n from the splitter's block metadata output port.\n\n\npublic class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator(\nInput\n, new JMSInput());\n    FileSplitterBase splitter = dag.addOperator(\nSplitter\n, new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator(\nBlockReader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nfile-info\n, input.output, splitter.input);\n    dag.addStream(\nblock-metadata\n, splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator\nAbstractFileSplitter.FileInfo\n\n  {\n\n    public final transient DefaultOutputPort\nAbstractFileSplitter.FileInfo\n output = new DefaultOutputPort\n();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}\n\n\n\n\nPorts\n\n\nDeclares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.\n\n\n\n\ninput: non optional port on which tuples of type \nFileInfo\n are received.\n\n\n\n\nConfiguration\n\n\n\n\nfile\n: path of the file from which the filesystem is inferred. FileSplitter creates an instance of \norg.apache.hadoop.fs.FileSystem\n which is why this path is needed.  \n\n\n\n\nFileSystem.newInstance(new Path(file).toUri(), new Configuration());\n\n\n\n\nThe fs instance is then used to fetch the default block size and \norg.apache.hadoop.fs.FileStatus\n for each file path.\n\n\nFileSplitterInput\n\n\nThis is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by \nTimeBasedDirectoryScanner\n. The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.\n\n\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.\n\n\n\n\nSplitter is the input operator here that sends block metadata to the downstream BlockReader.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nInput\n, new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator(\nBlock Reader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nblock-metadata\n, input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }\n\n\n\n\n\nPorts\n\n\nSince it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.\n\n\nConfiguration\n\n\n\n\nscanner\n: the component that scans directories asynchronously. It is of type \ncom.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner\n. The basic implementation of TimeBasedDirectoryScanner can be customized by users.  \n\n\n\n\na. \nfiles\n: comma separated list of directories to scan.  \n\n\nb. \nrecursive\n: flag that controls whether the directories should be scanned recursively.  \n\n\nc. \nscanIntervalMillis\n: interval specified in milliseconds after which another scan iteration is triggered.  \n\n\nd. \nfilePatternRegularExp\n: regular expression for accepted file names.  \n\n\ne. \ntrigger\n: a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2. \nidempotentStorageManager\n: by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of \ncom.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager\n.\n\n\nHandling of split records\n\n\nSplitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.\n\n\nWe have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers- \nAbstractFSLineReader\n and \nAbstractFSReadAheadLineReader\n can be found here \nAbstractFSBlockReader\n.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/file_splitter/#file-splitter", 
            "text": "This is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/file_splitter/#why-is-it-needed", 
            "text": "It is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/file_splitter/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/file_splitter/#abstractfilesplitter", 
            "text": "The abstract implementation defines the logic of processing  FileInfo . This comprises the following tasks -      building  FileMetadata  per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.    creating  BlockMetadataIterator  from  FileMetadata . The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.    retrieving  BlockMetadata.FileBlockMetadata  from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by  blocksThreshold  setting which by default is 1.      The main utility method that performs all the above tasks is the  process()  method. Concrete implementations can invoke this method whenever they have data to process.", 
            "title": "AbstractFileSplitter"
        }, 
        {
            "location": "/operators/file_splitter/#ports", 
            "text": "Declares only output ports on which file metadata and block metadata are emitted.   filesMetadataOutput: metadata for each file is emitted on this port.   blocksMetadataOutput: metadata for each block is emitted on this port.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#abstract-methods", 
            "text": "FileInfo getFileInfo() : called from within the  process()  and provides the next file to process.    long getDefaultBlockSize() : provides the block size which is used when user hasn't configured the size.    FileStatus getFileStatus(Path path) : provides the  org.apache.hadoop.fs.FileStatus  instance for a path.", 
            "title": "Abstract methods"
        }, 
        {
            "location": "/operators/file_splitter/#configuration", 
            "text": "blockSize : size of a block.  blocksThreshold : threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterbase", 
            "text": "Simple operator that receives tuples of type  FileInfo  on its  input  port.  FileInfo  contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.", 
            "title": "FileSplitterBase"
        }, 
        {
            "location": "/operators/file_splitter/#example-application", 
            "text": "This is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.  The upstream operator emits tuples of type  FileInfo  on its output port which is connected to splitter input port. The downstream receives tuples of type  BlockMetadata.FileBlockMetadata  from the splitter's block metadata output port.  public class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator( Input , new JMSInput());\n    FileSplitterBase splitter = dag.addOperator( Splitter , new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator( BlockReader , new FSSliceReader());\n    ...\n    dag.addStream( file-info , input.output, splitter.input);\n    dag.addStream( block-metadata , splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator AbstractFileSplitter.FileInfo \n  {\n\n    public final transient DefaultOutputPort AbstractFileSplitter.FileInfo  output = new DefaultOutputPort ();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}", 
            "title": "Example application"
        }, 
        {
            "location": "/operators/file_splitter/#ports_1", 
            "text": "Declares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.   input: non optional port on which tuples of type  FileInfo  are received.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#configuration_1", 
            "text": "file : path of the file from which the filesystem is inferred. FileSplitter creates an instance of  org.apache.hadoop.fs.FileSystem  which is why this path is needed.     FileSystem.newInstance(new Path(file).toUri(), new Configuration());  The fs instance is then used to fetch the default block size and  org.apache.hadoop.fs.FileStatus  for each file path.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterinput", 
            "text": "This is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by  TimeBasedDirectoryScanner . The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.", 
            "title": "FileSplitterInput"
        }, 
        {
            "location": "/operators/file_splitter/#example-application_1", 
            "text": "This is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.   Splitter is the input operator here that sends block metadata to the downstream BlockReader.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( Input , new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator( Block Reader , new FSSliceReader());\n    ...\n    dag.addStream( block-metadata , input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }", 
            "title": "Example application"
        }, 
        {
            "location": "/operators/file_splitter/#ports_2", 
            "text": "Since it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_splitter/#configuration_2", 
            "text": "scanner : the component that scans directories asynchronously. It is of type  com.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner . The basic implementation of TimeBasedDirectoryScanner can be customized by users.     a.  files : comma separated list of directories to scan.    b.  recursive : flag that controls whether the directories should be scanned recursively.    c.  scanIntervalMillis : interval specified in milliseconds after which another scan iteration is triggered.    d.  filePatternRegularExp : regular expression for accepted file names.    e.  trigger : a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2.  idempotentStorageManager : by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of  com.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager .", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/file_splitter/#handling-of-split-records", 
            "text": "Splitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.  We have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers-  AbstractFSLineReader  and  AbstractFSReadAheadLineReader  can be found here  AbstractFSBlockReader .", 
            "title": "Handling of split records"
        }, 
        {
            "location": "/operators/block_reader/", 
            "text": "Block Reader\n\n\nThis is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block. \n\n\nWhy is it needed?\n\n\nA Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see \nAbstractFileInputOperator\n) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.\n\n\nClass Diagram\n\n\n\n\nAbstractBlockReader\n\n\nThis is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.\n\n\n\n\nPorts\n\n\n\n\n\n\nblocksMetadataInput: input port on which block metadata are received.\n\n\n\n\n\n\nblocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.\n\n\n\n\n\n\nmessages: output port on which tuples of type \ncom.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord\n are emitted. This class encapsulates a \nrecord\n and the \nblockId\n of the corresponding block.\n\n\n\n\n\n\nreaderContext\n\n\nThis is one of the most important fields in the block reader. It is of type \ncom.datatorrent.lib.io.block.ReaderContext\n and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.\n\n\nOnce the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking \nreaderContext.initialize(stream, blockMetadata, consecutiveBlock);\n. Initialize method is where any implementation of \nReaderContext\n can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.\n\n\nOnce the initialization is done, \nreaderContext.next()\n is called repeatedly until it returns \nnull\n. It is left to the \nReaderContext\n implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples- \nLineReaderContext\n and \nReadAheadLineReaderContext\n). In other cases when there isn't a possibility of split record (example- \nFixedBytesReaderContext\n), it returns \nnull\n immediately when the block boundary is reached. The return type of \nreaderContext.next()\n is of type \ncom.datatorrent.lib.io.block.ReaderContext.Entity\n which is just a wrapper for a \nbyte[]\n that represents the record and total bytes used in fetching the record.\n\n\nAbstract methods\n\n\n\n\n\n\nSTREAM setupStream(B block)\n: creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.\n\n\n\n\n\n\nR convertToRecord(byte[] bytes)\n: this converts the array of bytes into the actual instance of record type.\n\n\n\n\n\n\nAuto-scalability\n\n\nBlock reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the \nblocksMetadataInput\n port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the \npartitioner and stats-listener\n.\n\n\nConfiguration\n\n\n\n\nmaxReaders\n: when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.\n\n\nminReaders\n: when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.\n\n\ncollectStats\n: this enables or disables auto-scaling. When it is set to \ntrue\n the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.\n\n\nintervalMillis\n: when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.\n\n\n\n\n AbstractFSBlockReader\n\n\nThis abstract implementation deals with files. Different types of file systems that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The user can override \ngetFSInstance()\n method to create an instance of a specific \nFileSystem\n. By default, filesystem instance is created from the filesytem URI that comes from the default hadoop configuration.\n\n\nprotected FileSystem getFSInstance() throws IOException\n{\n  return FileSystem.newInstance(configuration);\n}\n\n\n\n\nIt uses this filesystem instance to setup a stream of type \norg.apache.hadoop.fs.FSDataInputStream\n to read the block.\n\n\n@Override\nprotected FSDataInputStream setupStream(BlockMetadata.FileBlockMetadata block) throws IOException\n{\n  return fs.open(new Path(block.getFilePath()));\n}\n\n\n\n\nAll the ports and configurations are derived from the super class. It doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n method which is delegated to concrete sub-classes.\n\n\nExample Application\n\n\nThis simple dag demonstrates how any concrete implementation of \nAbstractFSBlockReader\n can be plugged into an application. \n\n\n\n\nIn the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.\n\n\npublic class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nFile-splitter\n, new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator(\nBlock-reader\n, new LineReader());\n    Filter filter = dag.addOperator(\nFilter\n, new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator(\nRecord-writer\n, new RecordOutputOperator());\n\n    dag.addStream(\nfile-block metadata\n, input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream(\nrecords\n, blockReader.messages, filter.input);\n    dag.addStream(\nfiltered-records\n, filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader\nString\n\n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort\nAbstractBlockReader.ReaderRecord\nString\n output = new DefaultOutputPort\n();\n    public final transient DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n input = new DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord\nString\n stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(), \n.\n)) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator\nAbstractBlockReader.ReaderRecord\nString\n\n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}\n\n\n\n\nConfiguration to parallel partition block reader with its downstream operators.\n\n\n  \nproperty\n\n    \nname\ndt.operator.Filter.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n\n\n\n\nAbstractFSReadAheadLineReader\n\n\nThis extension of \nAbstractFSBlockReader\n parses lines from a block and binds the \nreaderContext\n field to an instance of \nReaderContext.ReadAheadLineReaderContext\n.\n\n\nIt is abstract because it doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n since the user may want to convert the bytes that make a line into some other type. \n\n\nReadAheadLineReaderContext\n\n\nIn order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.\n\n\nThis is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.\n\n\nAbstractFSLineReader\n\n\nSimilar to \nAbstractFSReadAheadLineReader\n, even this parses lines from a block. However, it binds the \nreaderContext\n field to an instance of \nReaderContext.LineReaderContext\n.\n\n\nLineReaderContext\n\n\nThis handles the line split differently from \nReadAheadLineReaderContext\n. It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.\n\n\nWhen the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.\n\n\nIf the validations of completeness fails for a line then \nconvertToRecord(byte[] bytes)\n should return null.\n\n\nFSSliceReader\n\n\nA concrete extension of \nAbstractFSBlockReader\n that reads fixed-size \nbyte[]\n from a block and emits the byte array wrapped in \ncom.datatorrent.netlet.util.Slice\n.\n\n\nThis operator binds the \nreaderContext\n to an instance of \nReaderContext.FixedBytesReaderContext\n.\n\n\nFixedBytesReaderContext\n\n\nThis implementation of \nReaderContext\n never reads beyond a block boundary which can result in the last \nbyte[]\n of a block to be of a shorter length than the rest of the records.\n\n\nConfiguration\n\n\nreaderContext.length\n: length of each record. By default, this is initialized to the default hdfs block size.\n\n\nPartitioner and StatsListener\n\n\nThe logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute - \nPARTITIONER\n) as well as a StatsListener. This is because the \n\nAbstractBlockReader\n implements both the \ncom.datatorrent.api.Partitioner\n and \ncom.datatorrent.api.StatsListener\n interfaces and provides an implementation of \ndefinePartitions(...)\n and \nprocessStats(...)\n which make it auto-scalable.\n\n\nprocessStats \n\n\nThe application master invokes \nResponse processStats(BatchedOperatorStats stats)\n method on the logical instance with the stats (\ntuplesProcessedPSMA\n, \ntuplesEmittedPSMA\n, \nlatencyMA\n, etc.) of each partition. The data which this operator is interested in is the \nqueueSize\n of the input port \nblocksMetadataInput\n.\n\n\nUsually the \nqueueSize\n of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with \n@DataQueueSize\n. In this case \nAbstractBlockReader\n itself is the \nStatsListener\n which is why it is annotated with \n@DataQueueSize\n.\n\n\nThe logical instance caches the queue size per partition and at regular intervals (configured by \nintervalMillis\n) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.\n\n\n\n\nThe goal of this logic is to create as many partitions within bounds (see \nmaxReaders\n and \nminReaders\n above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.\n\n\ndefinePartitions\n\n\nBased on the \nrepartitionRequired\n field of the \nResponse\n object which is returned by \nprocessStats\n method, the application master invokes \n\n\nCollection\nPartition\nAbstractBlockReader\n...\n definePartitions(Collection\nPartition\nAbstractBlockReader\n...\n partitions, PartitioningContext context)\n\n\n\n\non the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created. \n\n\nPlease note auto-scaling can be disabled by setting \ncollectStats\n to \nfalse\n. If the use-case requires only static partitioning, then that can be achieved by setting \nStatelessPartitioner\n as the operator attribute- \nPARTITIONER\n on the block reader.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/block_reader/#block-reader", 
            "text": "This is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/block_reader/#why-is-it-needed", 
            "text": "A Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see  AbstractFileInputOperator ) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/block_reader/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/block_reader/#abstractblockreader", 
            "text": "This is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.", 
            "title": "AbstractBlockReader"
        }, 
        {
            "location": "/operators/block_reader/#ports", 
            "text": "blocksMetadataInput: input port on which block metadata are received.    blocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.    messages: output port on which tuples of type  com.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord  are emitted. This class encapsulates a  record  and the  blockId  of the corresponding block.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/block_reader/#readercontext", 
            "text": "This is one of the most important fields in the block reader. It is of type  com.datatorrent.lib.io.block.ReaderContext  and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.  Once the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking  readerContext.initialize(stream, blockMetadata, consecutiveBlock); . Initialize method is where any implementation of  ReaderContext  can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.  Once the initialization is done,  readerContext.next()  is called repeatedly until it returns  null . It is left to the  ReaderContext  implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples-  LineReaderContext  and  ReadAheadLineReaderContext ). In other cases when there isn't a possibility of split record (example-  FixedBytesReaderContext ), it returns  null  immediately when the block boundary is reached. The return type of  readerContext.next()  is of type  com.datatorrent.lib.io.block.ReaderContext.Entity  which is just a wrapper for a  byte[]  that represents the record and total bytes used in fetching the record.", 
            "title": "readerContext"
        }, 
        {
            "location": "/operators/block_reader/#abstract-methods", 
            "text": "STREAM setupStream(B block) : creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.    R convertToRecord(byte[] bytes) : this converts the array of bytes into the actual instance of record type.", 
            "title": "Abstract methods"
        }, 
        {
            "location": "/operators/block_reader/#auto-scalability", 
            "text": "Block reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the  blocksMetadataInput  port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the  partitioner and stats-listener .", 
            "title": "Auto-scalability"
        }, 
        {
            "location": "/operators/block_reader/#configuration", 
            "text": "maxReaders : when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.  minReaders : when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.  collectStats : this enables or disables auto-scaling. When it is set to  true  the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.  intervalMillis : when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/block_reader/#example-application", 
            "text": "This simple dag demonstrates how any concrete implementation of  AbstractFSBlockReader  can be plugged into an application.    In the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.  public class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( File-splitter , new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator( Block-reader , new LineReader());\n    Filter filter = dag.addOperator( Filter , new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator( Record-writer , new RecordOutputOperator());\n\n    dag.addStream( file-block metadata , input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream( records , blockReader.messages, filter.input);\n    dag.addStream( filtered-records , filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader String \n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort AbstractBlockReader.ReaderRecord String  output = new DefaultOutputPort ();\n    public final transient DefaultInputPort AbstractBlockReader.ReaderRecord String  input = new DefaultInputPort AbstractBlockReader.ReaderRecord String ()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord String  stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(),  . )) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator AbstractBlockReader.ReaderRecord String \n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord String  tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord String  tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}  Configuration to parallel partition block reader with its downstream operators.     property \n     name dt.operator.Filter.port.input.attr.PARTITION_PARALLEL /name \n     value true /value \n   /property \n   property \n     name dt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL /name \n     value true /value \n   /property", 
            "title": "Example Application"
        }, 
        {
            "location": "/operators/block_reader/#abstractfsreadaheadlinereader", 
            "text": "This extension of  AbstractFSBlockReader  parses lines from a block and binds the  readerContext  field to an instance of  ReaderContext.ReadAheadLineReaderContext .  It is abstract because it doesn't provide an implementation of  convertToRecord(byte[] bytes)  since the user may want to convert the bytes that make a line into some other type.", 
            "title": "AbstractFSReadAheadLineReader"
        }, 
        {
            "location": "/operators/block_reader/#readaheadlinereadercontext", 
            "text": "In order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.  This is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.", 
            "title": "ReadAheadLineReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#abstractfslinereader", 
            "text": "Similar to  AbstractFSReadAheadLineReader , even this parses lines from a block. However, it binds the  readerContext  field to an instance of  ReaderContext.LineReaderContext .", 
            "title": "AbstractFSLineReader"
        }, 
        {
            "location": "/operators/block_reader/#linereadercontext", 
            "text": "This handles the line split differently from  ReadAheadLineReaderContext . It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.  When the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.  If the validations of completeness fails for a line then  convertToRecord(byte[] bytes)  should return null.", 
            "title": "LineReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#fsslicereader", 
            "text": "A concrete extension of  AbstractFSBlockReader  that reads fixed-size  byte[]  from a block and emits the byte array wrapped in  com.datatorrent.netlet.util.Slice .  This operator binds the  readerContext  to an instance of  ReaderContext.FixedBytesReaderContext .", 
            "title": "FSSliceReader"
        }, 
        {
            "location": "/operators/block_reader/#fixedbytesreadercontext", 
            "text": "This implementation of  ReaderContext  never reads beyond a block boundary which can result in the last  byte[]  of a block to be of a shorter length than the rest of the records.", 
            "title": "FixedBytesReaderContext"
        }, 
        {
            "location": "/operators/block_reader/#configuration_1", 
            "text": "readerContext.length : length of each record. By default, this is initialized to the default hdfs block size.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operators/block_reader/#partitioner-and-statslistener", 
            "text": "The logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute -  PARTITIONER ) as well as a StatsListener. This is because the  AbstractBlockReader  implements both the  com.datatorrent.api.Partitioner  and  com.datatorrent.api.StatsListener  interfaces and provides an implementation of  definePartitions(...)  and  processStats(...)  which make it auto-scalable.", 
            "title": "Partitioner and StatsListener"
        }, 
        {
            "location": "/operators/block_reader/#processstats", 
            "text": "The application master invokes  Response processStats(BatchedOperatorStats stats)  method on the logical instance with the stats ( tuplesProcessedPSMA ,  tuplesEmittedPSMA ,  latencyMA , etc.) of each partition. The data which this operator is interested in is the  queueSize  of the input port  blocksMetadataInput .  Usually the  queueSize  of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with  @DataQueueSize . In this case  AbstractBlockReader  itself is the  StatsListener  which is why it is annotated with  @DataQueueSize .  The logical instance caches the queue size per partition and at regular intervals (configured by  intervalMillis ) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.   The goal of this logic is to create as many partitions within bounds (see  maxReaders  and  minReaders  above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.", 
            "title": "processStats "
        }, 
        {
            "location": "/operators/block_reader/#definepartitions", 
            "text": "Based on the  repartitionRequired  field of the  Response  object which is returned by  processStats  method, the application master invokes   Collection Partition AbstractBlockReader ...  definePartitions(Collection Partition AbstractBlockReader ...  partitions, PartitioningContext context)  on the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created.   Please note auto-scaling can be disabled by setting  collectStats  to  false . If the use-case requires only static partitioning, then that can be achieved by setting  StatelessPartitioner  as the operator attribute-  PARTITIONER  on the block reader.", 
            "title": "definePartitions"
        }, 
        {
            "location": "/operators/file_output/", 
            "text": "AbstractFileOutputOperator\n\n\nThe abstract file output operator in Apache Apex Malhar library \n \nAbstractFileOutputOperator\n writes streaming data to files. The main features of this operator are:\n\n\n\n\nPersisting data to files.\n\n\nAutomatic rotation of files based on:\n\n  a. maximum length of a file.\n\n  b. time-based rotation where time is specified using a count of application windows.\n\n\nFault-tolerance.\n\n\nCompression and encryption of data before it is persisted.\n\n\n\n\nIn this tutorial we will cover the details of the basic structure and implementation of all the above features in \nAbstractFileOutputOperator\n. Configuration items related to each feature are discussed as they are introduced in the section of that feature.\n\n\nPersisting data to files\n\n\nThe principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:\n\n\nfilePath\n: path specifying the directory where files are written.\n\n\nDifferent types of file system that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The file system instance which is used for creating streams is constructed from the \nfilePath\n URI.\n\n\nFileSystem.newInstance(new Path(filePath).toUri(), new Configuration())\n\n\n\n\nTuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.\n\n\nPorts\n\n\n\n\ninput\n: the input port on which tuples to be persisted are received.\n\n\n\n\nstreamsCache\n\n\nThis transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.\n\n\nstreamsCache\n is of type \ncom.google.common.cache.LoadingCache\n. A \nLoadingCache\n has an attached \nCacheLoader\n which is responsible to load value of a key when the key is not present in the cache. Details are explained here- \nCachesExplained\n.\n\n\nThe operator constructs this cache in \nsetup(...)\n. It is built with the following configuration items:\n\n\n\n\nmaxOpenFiles\n: maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit. \nDefault\n: 100\n\n\nexpireStreamAfterAcessMillis\n: expires streams after the specified duration has passed since the stream was last accessed. \nDefault\n: value of attribute- \nOperatorContext.SPIN_MILLIS\n.\n\n\n\n\nAn important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.\n\n\nCacheLoader\n\n\nstreamsCache\n is created with a \nCacheLoader\n that opens an \nFSDataOutputStream\n for a file which is not in the cache. The output stream is opened in either \nappend\n or \ncreate\n mode and the basic logic to determine this is explained by the simple diagram below.\n\n\n\n\nThis process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.\n\n\nFollowing are few configuration items used for opening the streams:\n\n\n\n\nreplication\n: specifies the replication factor of the output files. \nDefault\n: \nfs.getDefaultReplication(new Path(filePath))\n\n\nfilePermission\n: specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command. \nDefault\n: 0777\n\n\n\n\nRemovalListener\n\n\nA \nGuava\n cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since \nstreamsCache\n is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to \nstreamsCache\n which closes the stream when it is evicted.\n\n\nsetup(OperatorContext context)\n\n\nDuring setup the following main tasks are performed:\n\n\n\n\nFileSystem instance is created.\n\n\nThe cache of streams is created.\n\n\nFiles are recovered (see Fault-tolerance section).\n\n\nStray part files are cleaned (see Automatic rotation section).\n\n\n\n\nprocessTuple(INPUT tuple)\n\n\nThe code snippet below highlights the basic steps of processing a tuple.\n\n\nprotected void processTuple(INPUT tuple)\n{  \n  //which file to write to is derived from the tuple.\n  String fileName = getFileName(tuple);  \n\n  //streamsCache is queried for the output stream. If the stream is already opened then it is returned immediately otherwise the cache loader creates one.\n  FilterOutputStream fsOutput = streamsCache.get(fileName).getFilterStream();\n\n  byte[] tupleBytes = getBytesForTuple(tuple);\n\n  fsOutput.write(tupleBytes);\n}\n\n\n\n\nendWindow()\n\n\nIt should be noted that while processing a tuple we do not flush the stream after every write. Since flushing is expensive it is done periodically for all the open streams in the operator's \nendWindow()\n.\n\n\nMap\nString, FSFilterStreamContext\n openStreams = streamsCache.asMap();\nfor (FSFilterStreamContext streamContext: openStreams.values()) {\n  ...\n  //this flushes the stream\n  streamContext.finalizeContext();\n  ...\n}\n\n\n\n\nFSFilterStreamContext\n will be explained with compression and encryption.\n\n\nteardown()\n\n\nWhen any operator in a DAG fails then the application master invokes \nteardown()\n for that operator and its downstream operators. In \nAbstractFileOutputOperator\n we have a bunch of open streams in the cache and the operator (acting as HDFS client) holds leases for all the corresponding files. It is important to release these leases for clean re-deployment. Therefore, we try to close all the open streams in \nteardown()\n.\n\n\nAutomatic rotation\n\n\nIn a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.\n\n\nTo help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.\n\n\nPart filename\n\n\nThe filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,\n\n\norigfile.partnum\n\n\nThis naming scheme can be changed by the user. It can be done so by overriding the following method\n\n\nprotected String getPartFileName(String fileName, int part)\n\n\n\n\nThis method is passed the original filename and part number as arguments and should return the part filename.\n\n\nMechanisms\n\n\nThe user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.\n\n\nSize Based\n\n\nWith size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property\n\n\nmaxLength\n\n\nLike any other property this can be set in Java application code or in the property file.\n\n\nTime Based\n\n\nIn time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property\n\n\nrotationWindows\n\n\nsetup(OperatorContext context)\n\n\nWhen an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process\n\n\n\n\nFault-tolerance\n\n\nThere are two issues that should be addressed in order to make the operator fault-tolerant:\n\n\n\n\n\n\nThe operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.\n\n\n\n\n\n\nWhile writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:  \n\n\n\n\nalwaysWriteToTmp\n: enables/disables writing to a temporary file. \nDefault\n: true.\n\n\n\n\nMost of the complexity in the code comes from making this operator fault-tolerant.\n\n\nCheckpointed states needed for fault-tolerance\n\n\n\n\n\n\nendOffsets\n: contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator \nsetup(...)\n and is also used while loading a stream to find out if the operator has seen a file before.\n\n\n\n\n\n\nfileNameToTmpName\n: contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.\n\n\n\n\n\n\nfinalizedFiles\n: contains set of files which were requested to be finalized per window id.\n\n\n\n\n\n\nfinalizedPart\n: contains the latest \npart\n of each file which was requested to be finalized.\n\n\n\n\n\n\nThe use of \nfinalizedFiles\n and \nfinalizedPart\n are explained in detail under \nrequestFinalize(...)\n method.\n\n\nRecovering files\n\n\nWhen the operator is re-deployed, it checks in its \nsetup(...)\n method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the \nendOffsets\n. When it doesn't the operator truncates the file.\n\n\nFor example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.\n\n\nrequestFinalize(String fileName)\n\n\nWhen the operator is always writing to temporary files (in order to avoid HDFS Lease exceptions), then it is necessary to rename the temporary files to the actual files once it has been determined that the files are closed. This is refered to as \nfinalization\n of files and the method allows the user code to specify when a file is ready for finalization.\n\n\nIn this method, the requested file (or in the case of rotation \n all the file parts including the latest open part which have not yet been requested for finalization) are registered for finalization. Registration is basically adding the file names to \nfinalizedFiles\n state and updating \nfinalizedPart\n.\n\n\nThe process of \nfinalization\n of all the files which were requested till the window \nw\n is deferred till window \nw\n is committed. This is because until a window is committed it can be replayed after a failure which means that a file can be open for writing even after it was requested for finalization.\n\n\nWhen rotation is enabled, part files as and when they get completed are requested for finalization. However, when rotation is not enabled user code needs to invoke this method as the knowledge that when a file is closed is unknown to this abstract operator.", 
            "title": "File Output"
        }, 
        {
            "location": "/operators/file_output/#abstractfileoutputoperator", 
            "text": "The abstract file output operator in Apache Apex Malhar library    AbstractFileOutputOperator  writes streaming data to files. The main features of this operator are:   Persisting data to files.  Automatic rotation of files based on: \n  a. maximum length of a file. \n  b. time-based rotation where time is specified using a count of application windows.  Fault-tolerance.  Compression and encryption of data before it is persisted.   In this tutorial we will cover the details of the basic structure and implementation of all the above features in  AbstractFileOutputOperator . Configuration items related to each feature are discussed as they are introduced in the section of that feature.", 
            "title": "AbstractFileOutputOperator"
        }, 
        {
            "location": "/operators/file_output/#persisting-data-to-files", 
            "text": "The principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:  filePath : path specifying the directory where files are written.  Different types of file system that are implementations of  org.apache.hadoop.fs.FileSystem  are supported. The file system instance which is used for creating streams is constructed from the  filePath  URI.  FileSystem.newInstance(new Path(filePath).toUri(), new Configuration())  Tuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.", 
            "title": "Persisting data to files"
        }, 
        {
            "location": "/operators/file_output/#ports", 
            "text": "input : the input port on which tuples to be persisted are received.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/file_output/#streamscache", 
            "text": "This transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.  streamsCache  is of type  com.google.common.cache.LoadingCache . A  LoadingCache  has an attached  CacheLoader  which is responsible to load value of a key when the key is not present in the cache. Details are explained here-  CachesExplained .  The operator constructs this cache in  setup(...) . It is built with the following configuration items:   maxOpenFiles : maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit.  Default : 100  expireStreamAfterAcessMillis : expires streams after the specified duration has passed since the stream was last accessed.  Default : value of attribute-  OperatorContext.SPIN_MILLIS .   An important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.", 
            "title": "streamsCache"
        }, 
        {
            "location": "/operators/file_output/#cacheloader", 
            "text": "streamsCache  is created with a  CacheLoader  that opens an  FSDataOutputStream  for a file which is not in the cache. The output stream is opened in either  append  or  create  mode and the basic logic to determine this is explained by the simple diagram below.   This process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.  Following are few configuration items used for opening the streams:   replication : specifies the replication factor of the output files.  Default :  fs.getDefaultReplication(new Path(filePath))  filePermission : specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command.  Default : 0777", 
            "title": "CacheLoader"
        }, 
        {
            "location": "/operators/file_output/#removallistener", 
            "text": "A  Guava  cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since  streamsCache  is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to  streamsCache  which closes the stream when it is evicted.", 
            "title": "RemovalListener"
        }, 
        {
            "location": "/operators/file_output/#setupoperatorcontext-context", 
            "text": "During setup the following main tasks are performed:   FileSystem instance is created.  The cache of streams is created.  Files are recovered (see Fault-tolerance section).  Stray part files are cleaned (see Automatic rotation section).", 
            "title": "setup(OperatorContext context)"
        }, 
        {
            "location": "/operators/file_output/#automatic-rotation", 
            "text": "In a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.  To help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.", 
            "title": "Automatic rotation"
        }, 
        {
            "location": "/operators/file_output/#part-filename", 
            "text": "The filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,  origfile.partnum  This naming scheme can be changed by the user. It can be done so by overriding the following method  protected String getPartFileName(String fileName, int part)  This method is passed the original filename and part number as arguments and should return the part filename.", 
            "title": "Part filename"
        }, 
        {
            "location": "/operators/file_output/#mechanisms", 
            "text": "The user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.", 
            "title": "Mechanisms"
        }, 
        {
            "location": "/operators/file_output/#size-based", 
            "text": "With size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property  maxLength  Like any other property this can be set in Java application code or in the property file.", 
            "title": "Size Based"
        }, 
        {
            "location": "/operators/file_output/#time-based", 
            "text": "In time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property  rotationWindows", 
            "title": "Time Based"
        }, 
        {
            "location": "/operators/file_output/#setupoperatorcontext-context_1", 
            "text": "When an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process", 
            "title": "setup(OperatorContext context)"
        }, 
        {
            "location": "/operators/file_output/#fault-tolerance", 
            "text": "There are two issues that should be addressed in order to make the operator fault-tolerant:    The operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.    While writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:     alwaysWriteToTmp : enables/disables writing to a temporary file.  Default : true.   Most of the complexity in the code comes from making this operator fault-tolerant.", 
            "title": "Fault-tolerance"
        }, 
        {
            "location": "/operators/file_output/#checkpointed-states-needed-for-fault-tolerance", 
            "text": "endOffsets : contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator  setup(...)  and is also used while loading a stream to find out if the operator has seen a file before.    fileNameToTmpName : contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.    finalizedFiles : contains set of files which were requested to be finalized per window id.    finalizedPart : contains the latest  part  of each file which was requested to be finalized.    The use of  finalizedFiles  and  finalizedPart  are explained in detail under  requestFinalize(...)  method.", 
            "title": "Checkpointed states needed for fault-tolerance"
        }, 
        {
            "location": "/operators/file_output/#recovering-files", 
            "text": "When the operator is re-deployed, it checks in its  setup(...)  method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the  endOffsets . When it doesn't the operator truncates the file.  For example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.", 
            "title": "Recovering files"
        }, 
        {
            "location": "/operators/enricher/", 
            "text": "POJO Enricher\n\n\nOperator Objective\n\n\nThis operator receives an POJO (\nPlain Old Java Object\n) as an incoming tuple and uses an external source to enrich the data in \nthe incoming tuple and finally emits the enriched data as a new enriched POJO.\n\n\nPOJOEnricher supports enrichment from following external sources:\n\n\n\n\nJSON File Based\n - Reads the file in memory having content stored in JSON format and use that to enrich the data. This can be done using FSLoader implementation.\n\n\nJDBC Based\n - Any JDBC store can act as an external entity to which enricher can request data for enriching incoming tuples. This can be done using JDBCLoader implementation.\n\n\n\n\nPOJO Enricher does not hold any state and is \nidempotent\n, \nfault-tolerant\n and \nstatically/dynamically partitionable\n.\n\n\nOperator Usecase\n\n\n\n\nBank \ntransaction records\n usually contains customerId. For further analysis of transaction one wants the customer name and other customer related information. \nSuch information is present in another database. One could enrich the transaction's record with customer information using POJOEnricher.\n\n\nCall Data Record (CDR)\n contains only mobile/telephone numbers of the customer. Customer information is missing in CDR. POJO Enricher can be used to enrich \nCDR with customer data for further analysis.\n\n\n\n\nOperator Information\n\n\n\n\nOperator location: \nmalhar-contrib\n\n\nAvailable since: \n3.4.0\n\n\nOperator state: \nEvolving\n\n\nJava Packages:\n\n\nOperator: \ncom.datatorrent.contrib.enrich.POJOEnricher\n\n\nFSLoader: \ncom.datatorrent.contrib.enrich.FSLoader\n\n\nJDBCLoader: \ncom.datatorrent.contrib.enrich.JDBCLoader\n\n\n\n\n\n\n\n\nProperties, Attributes and Ports\n\n\nProperties of POJOEnricher\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nMandatory\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nincludeFields\n\n\nList of fields from database that needs to be added to output POJO.\n\n\nList\nString>\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\nlookupFields\n\n\nList of fields from input POJO which will form a \nunique composite\n key for querying to store\n\n\nList\nString>\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\nstore\n\n\nBackend Store from which data should be queried for enrichment\n\n\nBackendStore\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\ncacheExpirationInterval\n\n\nCache entry expiry in ms. After this time, the lookup to store will be done again for given key\n\n\nint\n\n\nNo\n\n\n1 * 60 * 60 * 1000 (1 hour)\n\n\n\n\n\n\ncacheCleanupInterval\n\n\nInterval in ms after which cache will be removed for any stale entries.\n\n\nint\n\n\nNo\n\n\n1 * 60 * 60 * 1000 (1 hour)\n\n\n\n\n\n\ncacheSize\n\n\nNumber of entry in cache after which eviction will start on each addition based on LRU\n\n\nint\n\n\nNo\n\n\n1000\n\n\n\n\n\n\n\n\nProperties of FSLoader (BackendStore)\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nMandatory\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nfileName\n\n\nPath of the file, the data from which will be used for enrichment. See \nhere\n for JSON File format.\n\n\nString\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\n\n\nProperties of JDBCLoader (BackendStore)\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nType\n\n\nMandatory\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\ndatabaseUrl\n\n\nConnection string for connecting to JDBC\n\n\nString\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\ndatabaseDriver\n\n\nJDBC Driver class for connection to JDBC Store. This driver should be there in classpath\n\n\nString\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\ntableName\n\n\nName of the table from which data needs to be retrieved\n\n\nString\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\nconnectionProperties\n\n\nCommand seperated list of advanced connection properties that need to be passed to JDBC Driver. For eg. \nprop1:val1,prop2:val2\n\n\nString\n\n\nNo\n\n\nnull\n\n\n\n\n\n\nqueryStmt\n\n\nSelect statement which will be used to query the data. This is optional parameter in case of advanced query.\n\n\nString\n\n\nNo\n\n\nnull\n\n\n\n\n\n\n\n\nPlatform Attributes that influences operator behavior\n\n\n\n\n\n\n\n\nAttribute\n\n\nDescription\n\n\nType\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\ninput.TUPLE_CLASS\n\n\nTUPLE_CLASS attribute on input port which tells operator the class of POJO which will be incoming\n\n\nClass or FQCN\n\n\nYes\n\n\n\n\n\n\noutput.TUPLE_CLASS\n\n\nTUPLE_CLASS attribute on output port which tells operator the class of POJO which need to be emitted\n\n\nClass or FQCN\n\n\nYes\n\n\n\n\n\n\n\n\nPorts\n\n\n\n\n\n\n\n\nPort\n\n\nDescription\n\n\nType\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\ninput\n\n\nTuple which needs to be enriched are received on this port\n\n\nObject (POJO)\n\n\nYes\n\n\n\n\n\n\noutput\n\n\nTuples that are enriched from external source are emitted from on this port\n\n\nObject (POJO)\n\n\nNo\n\n\n\n\n\n\n\n\nLimitations\n\n\nCurrent POJOEnricher contains following limitation:\n\n\n\n\nFSLoader loads the file content in memory. Though it loads only the composite key and composite value in memory, a very large amount of data would bloat the memory and make the operator go OOM. In case the filesize is large, allocate sufficient memory to the POJOEnricher.\n\n\nIncoming POJO should be a subset of outgoing POJO.\n\n\nincludeFields\n property should contains fields having same name in database column as well as outgoing POJO. For eg. If name of the database column is \"customerName\", then outgoing POJO should contains a field with the same name and same should be added to includeFields.\n\n\nlookupFields\n property should contains fields having same name in database column as well as incoming POJO. For eg. If name of the database column is \"customerId\", then incoming POJO should contains a field with the same name and same should be added to lookupFields.\n\n\n\n\nExample\n\n\nExample for POJOEnricher can be found at: \nhttps://github.com/DataTorrent/examples/tree/master/tutorials/enricher\n\n\nAdvanced\n\n\n File format for JSON based FSLoader\n\n\nFSLoader expects file to be in specific format:\n\n\n\n\nEach line makes on record which becomes part of the store\n\n\nEach line is a valid JSON Object where \nkey\n is name of the field name and \nvalue\n is the field value.\n\n\n\n\nExample for the format look like following:\n\n\n{\ncircleId\n:0, \ncircleName\n:\nA\n}\n{\ncircleId\n:1, \ncircleName\n:\nB\n}\n{\ncircleId\n:2, \ncircleName\n:\nC\n}\n{\ncircleId\n:3, \ncircleName\n:\nD\n}\n{\ncircleId\n:4, \ncircleName\n:\nE\n}\n{\ncircleId\n:5, \ncircleName\n:\nF\n}\n{\ncircleId\n:6, \ncircleName\n:\nG\n}\n{\ncircleId\n:7, \ncircleName\n:\nH\n}\n{\ncircleId\n:8, \ncircleName\n:\nI\n}\n{\ncircleId\n:9, \ncircleName\n:\nJ\n}\n\n\n\n\nCaching mechanism in POJOEnricher\n\n\nPOJOEnricher contains an cache which makes the lookup for keys more efficient. This is specially useful when data in external store is not changing much. \nHowever, one should carefully tune the \ncacheExpirationInterval\n property for desirable results.\n\n\nOn every incoming tuple, POJOEnricher first queries the cache. If the cache contains desired record and is within expiration interval, then it uses that to\nenrich the tuple, otherwise does a lookup to configured store and the return value is used to enrich the tuple. The return value is then cached for composite key and composite value.\n\n\nPOJOEnricher only caches the required fields for enrichment mechanism and not all fields returned by external store. This ensures optimal use of memory.\n\n\nPartitioning of POJOEnricher\n\n\nBeing stateless operator, POJOEnricher will ensure built-in partitioners present in Malhar library can be directly simply by setting few properties as follows:\n\n\nStateless partioning of POJOEnricher\n\n\nStateless partitioning will ensure that POJOEnricher will will be partitioned right at the starting of the application and will remain partitioned throughout the lifetime of the DAG.\nPOJOEnricher can be stateless partitioned by adding following lines to properties.xml:\n\n\n  \nproperty\n\n    \nname\ndt.operator.{OperatorName}.attr.PARTITIONER\n/name\n\n    \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:2\n/value\n\n  \n/property\n\n\n\n\n\nwhere {OperatorName} is the name of the POJOEnricher operator.\nAbove lines will partition POJOEnricher statically 2 times. Above value can be changed accordingly to change the number of static partitions.\n\n\nDynamic Partitioning of POJOEnricher\n\n\nDynamic partitioning is a feature of Apex platform which changes the partition of the operator based on certain condition.\nPOJOEnricher can be dynamically partitioned using 2 out-of-the-box partitioners:\n\n\nThroughput based\n\n\nFollowing code can be added to populateDAG method of application to dynamically partitioning POJOEnricher:\n\n\n    StatelessThroughputBasedPartitioner\nPOJOEnricher\n partitioner = new StatelessThroughputBasedPartitioner\n();\n    partitioner.setCooldownMillis(conf.getLong(COOL_DOWN_MILLIS, 10000));\n    partitioner.setMaximumEvents(conf.getLong(MAX_THROUGHPUT, 30000));\n    partitioner.setMinimumEvents(conf.getLong(MIN_THROUGHPUT, 10000));\n    dag.setAttribute(pojoEnricherObj, OperatorContext.STATS_LISTENERS, Arrays.asList(new StatsListener[]{partitioner}));\n    dag.setAttribute(pojoEnricherObj, OperatorContext.PARTITIONER, partitioner);\n\n\n\n\nAbove code will dynamically partition POJOEnricher when the throughput changes.\nIf the overall throughput of POJOEnricher goes beyond 30000 or less than 10000, the platform will repartition POJOEnricher \nto balance throughput of a single partition to be between 10000 and 30000.\nCooldownMillis of 10000 will be used as the threshold time for which the throughout change is observed.\n\n\nLatency based\n\n\nFollowing code can be added to populateDAG method of application to dynamically partitioning POJOEnricher:\n\n\n    StatelessLatencyBasedPartitioner\nPOJOEnricher\n partitioner = new StatelessLatencyBasedPartitioner\n();\n    partitioner.setCooldownMillis(conf.getLong(COOL_DOWN_MILLIS, 10000));\n    partitioner.setMaximumLatency(conf.getLong(MAX_THROUGHPUT, 10));\n    partitioner.setMinimumLatency(conf.getLong(MIN_THROUGHPUT, 3));\n    dag.setAttribute(pojoEnricherObj, OperatorContext.STATS_LISTENERS, Arrays.asList(new StatsListener[]{partitioner}));\n    dag.setAttribute(pojoEnricherObj, OperatorContext.PARTITIONER, partitioner);\n\n\n\n\nAbove code will dynamically partition POJOEnricher when the overall latency of POJOEnricher changes.\nIf the overall latency of POJOEnricher goes beyond 10 ms or less than 3 ms, the platform will repartition POJOEnricher \nto balance latency of a single partition to be between 3 ms and 10 ms.\nCooldownMillis of 10000 will be used as the threshold time for which the latency change is observed.", 
            "title": "Enricher"
        }, 
        {
            "location": "/operators/enricher/#pojo-enricher", 
            "text": "", 
            "title": "POJO Enricher"
        }, 
        {
            "location": "/operators/enricher/#operator-objective", 
            "text": "This operator receives an POJO ( Plain Old Java Object ) as an incoming tuple and uses an external source to enrich the data in \nthe incoming tuple and finally emits the enriched data as a new enriched POJO.  POJOEnricher supports enrichment from following external sources:   JSON File Based  - Reads the file in memory having content stored in JSON format and use that to enrich the data. This can be done using FSLoader implementation.  JDBC Based  - Any JDBC store can act as an external entity to which enricher can request data for enriching incoming tuples. This can be done using JDBCLoader implementation.   POJO Enricher does not hold any state and is  idempotent ,  fault-tolerant  and  statically/dynamically partitionable .", 
            "title": "Operator Objective"
        }, 
        {
            "location": "/operators/enricher/#operator-usecase", 
            "text": "Bank  transaction records  usually contains customerId. For further analysis of transaction one wants the customer name and other customer related information. \nSuch information is present in another database. One could enrich the transaction's record with customer information using POJOEnricher.  Call Data Record (CDR)  contains only mobile/telephone numbers of the customer. Customer information is missing in CDR. POJO Enricher can be used to enrich \nCDR with customer data for further analysis.", 
            "title": "Operator Usecase"
        }, 
        {
            "location": "/operators/enricher/#operator-information", 
            "text": "Operator location:  malhar-contrib  Available since:  3.4.0  Operator state:  Evolving  Java Packages:  Operator:  com.datatorrent.contrib.enrich.POJOEnricher  FSLoader:  com.datatorrent.contrib.enrich.FSLoader  JDBCLoader:  com.datatorrent.contrib.enrich.JDBCLoader", 
            "title": "Operator Information"
        }, 
        {
            "location": "/operators/enricher/#properties-attributes-and-ports", 
            "text": "", 
            "title": "Properties, Attributes and Ports"
        }, 
        {
            "location": "/operators/enricher/#properties-of-jdbcloader-backendstore", 
            "text": "Property  Description  Type  Mandatory  Default Value      databaseUrl  Connection string for connecting to JDBC  String  Yes  N/A    databaseDriver  JDBC Driver class for connection to JDBC Store. This driver should be there in classpath  String  Yes  N/A    tableName  Name of the table from which data needs to be retrieved  String  Yes  N/A    connectionProperties  Command seperated list of advanced connection properties that need to be passed to JDBC Driver. For eg.  prop1:val1,prop2:val2  String  No  null    queryStmt  Select statement which will be used to query the data. This is optional parameter in case of advanced query.  String  No  null", 
            "title": "Properties of JDBCLoader (BackendStore)"
        }, 
        {
            "location": "/operators/enricher/#platform-attributes-that-influences-operator-behavior", 
            "text": "Attribute  Description  Type  Mandatory      input.TUPLE_CLASS  TUPLE_CLASS attribute on input port which tells operator the class of POJO which will be incoming  Class or FQCN  Yes    output.TUPLE_CLASS  TUPLE_CLASS attribute on output port which tells operator the class of POJO which need to be emitted  Class or FQCN  Yes", 
            "title": "Platform Attributes that influences operator behavior"
        }, 
        {
            "location": "/operators/enricher/#ports", 
            "text": "Port  Description  Type  Mandatory      input  Tuple which needs to be enriched are received on this port  Object (POJO)  Yes    output  Tuples that are enriched from external source are emitted from on this port  Object (POJO)  No", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/enricher/#limitations", 
            "text": "Current POJOEnricher contains following limitation:   FSLoader loads the file content in memory. Though it loads only the composite key and composite value in memory, a very large amount of data would bloat the memory and make the operator go OOM. In case the filesize is large, allocate sufficient memory to the POJOEnricher.  Incoming POJO should be a subset of outgoing POJO.  includeFields  property should contains fields having same name in database column as well as outgoing POJO. For eg. If name of the database column is \"customerName\", then outgoing POJO should contains a field with the same name and same should be added to includeFields.  lookupFields  property should contains fields having same name in database column as well as incoming POJO. For eg. If name of the database column is \"customerId\", then incoming POJO should contains a field with the same name and same should be added to lookupFields.", 
            "title": "Limitations"
        }, 
        {
            "location": "/operators/enricher/#example", 
            "text": "Example for POJOEnricher can be found at:  https://github.com/DataTorrent/examples/tree/master/tutorials/enricher", 
            "title": "Example"
        }, 
        {
            "location": "/operators/enricher/#advanced", 
            "text": "", 
            "title": "Advanced"
        }, 
        {
            "location": "/operators/enricher/#caching-mechanism-in-pojoenricher", 
            "text": "POJOEnricher contains an cache which makes the lookup for keys more efficient. This is specially useful when data in external store is not changing much. \nHowever, one should carefully tune the  cacheExpirationInterval  property for desirable results.  On every incoming tuple, POJOEnricher first queries the cache. If the cache contains desired record and is within expiration interval, then it uses that to\nenrich the tuple, otherwise does a lookup to configured store and the return value is used to enrich the tuple. The return value is then cached for composite key and composite value.  POJOEnricher only caches the required fields for enrichment mechanism and not all fields returned by external store. This ensures optimal use of memory.", 
            "title": "Caching mechanism in POJOEnricher"
        }, 
        {
            "location": "/operators/enricher/#partitioning-of-pojoenricher", 
            "text": "Being stateless operator, POJOEnricher will ensure built-in partitioners present in Malhar library can be directly simply by setting few properties as follows:", 
            "title": "Partitioning of POJOEnricher"
        }, 
        {
            "location": "/operators/enricher/#stateless-partioning-of-pojoenricher", 
            "text": "Stateless partitioning will ensure that POJOEnricher will will be partitioned right at the starting of the application and will remain partitioned throughout the lifetime of the DAG.\nPOJOEnricher can be stateless partitioned by adding following lines to properties.xml:     property \n     name dt.operator.{OperatorName}.attr.PARTITIONER /name \n     value com.datatorrent.common.partitioner.StatelessPartitioner:2 /value \n   /property   where {OperatorName} is the name of the POJOEnricher operator.\nAbove lines will partition POJOEnricher statically 2 times. Above value can be changed accordingly to change the number of static partitions.", 
            "title": "Stateless partioning of POJOEnricher"
        }, 
        {
            "location": "/operators/enricher/#dynamic-partitioning-of-pojoenricher", 
            "text": "Dynamic partitioning is a feature of Apex platform which changes the partition of the operator based on certain condition.\nPOJOEnricher can be dynamically partitioned using 2 out-of-the-box partitioners:", 
            "title": "Dynamic Partitioning of POJOEnricher"
        }, 
        {
            "location": "/operators/enricher/#throughput-based", 
            "text": "Following code can be added to populateDAG method of application to dynamically partitioning POJOEnricher:      StatelessThroughputBasedPartitioner POJOEnricher  partitioner = new StatelessThroughputBasedPartitioner ();\n    partitioner.setCooldownMillis(conf.getLong(COOL_DOWN_MILLIS, 10000));\n    partitioner.setMaximumEvents(conf.getLong(MAX_THROUGHPUT, 30000));\n    partitioner.setMinimumEvents(conf.getLong(MIN_THROUGHPUT, 10000));\n    dag.setAttribute(pojoEnricherObj, OperatorContext.STATS_LISTENERS, Arrays.asList(new StatsListener[]{partitioner}));\n    dag.setAttribute(pojoEnricherObj, OperatorContext.PARTITIONER, partitioner);  Above code will dynamically partition POJOEnricher when the throughput changes.\nIf the overall throughput of POJOEnricher goes beyond 30000 or less than 10000, the platform will repartition POJOEnricher \nto balance throughput of a single partition to be between 10000 and 30000.\nCooldownMillis of 10000 will be used as the threshold time for which the throughout change is observed.", 
            "title": "Throughput based"
        }, 
        {
            "location": "/operators/enricher/#latency-based", 
            "text": "Following code can be added to populateDAG method of application to dynamically partitioning POJOEnricher:      StatelessLatencyBasedPartitioner POJOEnricher  partitioner = new StatelessLatencyBasedPartitioner ();\n    partitioner.setCooldownMillis(conf.getLong(COOL_DOWN_MILLIS, 10000));\n    partitioner.setMaximumLatency(conf.getLong(MAX_THROUGHPUT, 10));\n    partitioner.setMinimumLatency(conf.getLong(MIN_THROUGHPUT, 3));\n    dag.setAttribute(pojoEnricherObj, OperatorContext.STATS_LISTENERS, Arrays.asList(new StatsListener[]{partitioner}));\n    dag.setAttribute(pojoEnricherObj, OperatorContext.PARTITIONER, partitioner);  Above code will dynamically partition POJOEnricher when the overall latency of POJOEnricher changes.\nIf the overall latency of POJOEnricher goes beyond 10 ms or less than 3 ms, the platform will repartition POJOEnricher \nto balance latency of a single partition to be between 3 ms and 10 ms.\nCooldownMillis of 10000 will be used as the threshold time for which the latency change is observed.", 
            "title": "Latency based"
        }, 
        {
            "location": "/operators/deduper/", 
            "text": "Deduper - Operator Documentation\n\n\nIntroduction\n\n\nAbout this document\n\n\nThis document is intended as a guide for understanding and using\nthe Dedup operator.\n\n\nTerminology\n\n\nWe will refer to this operator as the Deduper or Dedup operator\ninterchangeably.\n\n\nOverview\n\n\nDedup - \u201cWhat\u201d in a Nutshell\n\n\nDedup is actually short for Deduplication. Duplicates are omnipresent and\ncan be found in almost any kind of data. Most of the times it is\nessential to discard, or at the very least separate out the data into\nunique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one of which contains all unique tuples, and the other which are\noccurring more than once in the original data set.\n\n\n\n\nDedup - \u201cHow\u201d in a Nutshell\n\n\nIn order to quickly decide whether an incoming tuple is duplicate\nor unique, it has to store each incoming tuple (or a signature, like key,\nfor example) to be used for comparison later. A plain in-memory storage\nmay work for small datasets, but will not scale for large ones. Deduper employs a large scale distributed persistent hashing mechanism (known as the Managed State) which allows\nit to identify if a particular tuple is duplicate or unique. Managed state is a layer on HDFS which allows all the stored data to be persisted in a distributed fashion.\nEach time it identifies a tuple as a unique tuple, it also\nstores it into the Managed state for future\nlookup.\n\n\n\n\nFollowing are the different components of the Deduper\n\n\n\n\nDedup Operator\n - This is responsible for the overall\n    functionality of the operator. This in turn makes use of other\n    components to establish the end goal of deciding whether a tuple is\n    a duplicate of some earlier tuple, or is a unique tuple.\n\n\nManaged State\n - Since, all of the data cannot be stored in\n    memory, this component allows us to persist existing unique keys on\n    HDFS in form of buckets. This is also responsible for fetching data as\n    requested by the Deduper. Since, it communicates with the HDFS, data access is slow and so it allows for asynchronous (non-blocking) calls to fetch data. This ensures that the Deduper is not blocked and can continue to process other tuples. It also supports an in-memory cache where it stores the fetched data so that repeated access to the same data is faster. Periodically, based on configuration, this also\n    discards data which is no longer needed.\n\n\n\n\nThis was a very basic introduction to the functioning of the\nDeduper. Following sections will go into more detail on each of the\ncomponents.\n\n\nUse cases - Basic Dedup\n\n\nDedup Key\n\n\nA dedup key is a set of one or more fields in the data tuple which\nacts as the key\u00a0for the tuples.\nThis is used by the deduper to compare tuples to arrive at the\nconclusion on whether two tuples are duplicates.\n\n\nConsider an example schema and two sample tuples\n\n\n{Name, Phone, Email, Date, State, Zip, Country}\n\n\nTuple 1:\n\n\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}\n\n\n\n\nTuple 2:\n\n\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}\n\n\n\n\nLet us assume that the Dedup Key\nis \n{Name, Phone}\n. In\nthis case, the two tuples are duplicates because the key fields are same\nin both the tuples. However, if the Dedup Key is {Phone,Email},\nthe two are unique as the email values differ.\n\n\nUse case Details\n\n\nConsider the case of de-duplicating a master data set\nwhich is stored in a file.\u00a0Further also consider the\nfollowing schema for tuples in the data set.\n\n\n{Name, Phone, Email, Date, City, Zip, Country}\n\n\nAlso consider that we need to identify unique customers from the\nmaster data set. So, ultimately the output needed for the use case is\ntwo data sets - Unique Records\u00a0and Duplicate Records.\n\n\nAs part of configuring the operator for this use case, we need to\nset the following parameters:\n\n\n\n\nkeyExpression\n\u00a0- This can be set as\n    the primary key which can be used to uniquely identify a Customer.\n    For example, we can set it to \nName,Email\n\n\n\n\nThe above configuration is sufficient to address this use case.\n\n\nUse case - Dedup with Expiry\n\n\nMotivation\n\n\nThe Basic Dedup use case is the most straightforward and is\nusually applied when the amount of data to be processed is not huge.\nHowever, if the incoming data is huge, or even never-ending, it is\nusually not necessary to keep storing all the data. This is because in\nmost real world use cases, the duplicates occur only a short distance\napart. Hence, after a while, it is usually okay to forget part of\nthe history and consider only limited history for identifying\nduplicates, in the interest of efficiency. In other words, we expire\n(ignore) some tuples which are (or were supposed to be) delivered long\nback. Doing so, reduces the load on the storage mechanism (managed state) which effectively deletes part of the history, thus making the whole process more\nefficient. We call this use case, Dedup with expiry.\n\n\nExpiry Key\n\n\nThe easiest way to understand this use case is to consider\ntime\u00a0as the criterion for expiring\ntuples. Time\u00a0is a natural expiry\nkey and is in line with the concept of expiry. Formally, an expiry field\nis a field in the input tuple which can be used to discard incoming\ntuples as expired. This expiry key\nusually works with another parameter called Expiry Period defined\nnext.\n\n\nExpiry Period\n\n\nThe expiry period is the value supplied by the user to define the\nextent of history which should be considered while expiring\ntuples.\n\n\nUse case Details\n\n\nConsider an incoming stream of system logs. The use case requires\nus to identify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is\n\n30-12-2014 00:00:00\n and the\nlatest message that the system has encountered had the time stamp\n\n31-12-2014 00:00:00\n, then the\nincoming message must be considered as expired. However, if the incoming\nmessage had any timestamp like \n30-12-2014\n00:11:00\n, it must be accepted into the system and be checked for a possible duplicate.\n\n\nThe expiry facet in the use case above gives us an advantage in\nthat we do not have to compare the incoming record with all\u00a0the data to check if it is a duplicate.\nAt the same time, all\u00a0the\nincoming data need not be stored; just a day worth of data is adequate to address the above use case.\n\n\nConfiguring the below parameters will solve the problem for this\nuse case:\n\n\n\n\nkeyExpression\n\u00a0- This is the dedup key for the incoming tuples (similar to the Basic Dedup use case). This can be any key which can uniquely identify a record. For log messages this can be a serial number attached in the log.\n\n\ntimeExpression\n\u00a0- This is the key which can help identify the expired records, as explained above. In this particular use case, it can be a timestamp field which indicates when the log message was generated.\n\n\nexpireBefore\n\u00a0- This is the period of expiry as explained above. In our example use case this will be 24 hour, specified in seconds.\n\n\n\n\nConfiguration of the above parameters is sufficient to address this use\ncase.\n\n\nUse cases - Summary\n\n\n\n\nBasic Dedup\n - Deduplication of\n    bounded datasets. Data is assumed to be bounded. This use case is\n    not meant for never ending streams of data. For example:\n    Deduplication of master data like customer records, product catalogs\n    etc.\n\n\nTime Based Dedup\n\u00a0- Deduplication of\n    unlimited streams of data. This use case handles unbounded streams\n    of data and can run forever. An expiry key and criterion is expected\n    as part of the input which helps avoid storing all the unique data.\n    This helps speed up performance. Any timestamp field in the incoming\n    tuple can be used as a time based expiry key.\n\n\nWith respect to system time\n\u00a0- Time progresses with system time. Any expiry criteria are executed with the notion of system time. This is possible if the incoming tuple does not have a time field, or the user does not specify a \ntimeExpression\n.\n\n\nWith respect to tuple time\n\u00a0- Time progresses based on the time in the incoming tuples. Expiry criteria are executed with the notion of time indicated by the incoming tuple. Specification of the time field (\ntimeExpression\n) is mandatory for this scenario.\n\n\n\n\n\n\n\n\nTechnical Architecture\n\n\nClass Structure\n\n\n\n\n\n\nArchitectural Details\n\n\n\n\nConcepts\n\n\nDedup Key - Specified by \nkeyExpression\n\u00a0parameter\n\n\nA dedup key is a set of one or more fields in the data tuple which\nacts as the key\u00a0for the tuples.\nThis is used by the deduper to compare tuples to arrive at the\nconclusion on whether two tuples are duplicates. If Dedup Key of two\ntuples match, then they are duplicates, else they are unique.\n\n\nExpiry Key - Specified by \ntimeExpression\n\u00a0parameter\n\n\nA tuple may or may not have an Expiry Key. Dedup operator cannot\nkeep storing all the data that is flowing into the operator. At some\npoint it becomes essential to discard some of the historical tuples in\ninterest of memory and efficiency.\n\n\nAt the same time, tuples are expected to arrive at the Dedup\noperator within some time after they are generated. After this time, the\ntuples may be considered as stale or obsolete.\n\n\nIn such cases, the Deduper considers these tuples as\nexpired\u00a0and takes no action other than\nseparating out these tuples on a different port in order to be processed\nby some other operator or stored offline for analysis.\n\n\nIn order to create a criterion for discarding such tuples, we\nintroduce an Expiry Key. Looking at the value of the Expiry Key in each\ntuple, we can decide whether or not to discard this tuple as\nexpired.\n\n\nThe expiry key that we consider in Time Based Dedup is\ntime. This usually works with\nanother parameter called Expiry Period defined next.\n\n\nExpiry Period\n\n\nThe Expiry Period is the value supplied by the user which decides\nwhen a particular tuple expires.\n\n\nTime Points\n\n\nFor every dataset that the deduper processes, a set of time points is maintained:\n\n\n\n\nLatest Point\n\u00a0- This is the maximum\n    time point observed in all the processed tuples.\n\n\nExpiry Point\n\u00a0- This is given by:\n    \nExpiry Point = Latest Point - Expiry Period\n\n\n\n\nThese points help the deduper to make decisions related to expiry\nof a tuple.\n\n\nExample - Expiry\n\n\n\n\n\n\n\n\nTuple Id\n\n\nExpiry Key (Expiry Period = 10)\n\n\nLatest Point\n\n\nExpiry Point\n\n\nDecision for Tuple\n\n\n\n\n\n\n\n\n\n\n1\n\n\n10\n\n\n10\n\n\n1\n\n\nNot Expired\n\n\n\n\n\n\n2\n\n\n20\n\n\n20\n\n\n11\n\n\nNot Expired\n\n\n\n\n\n\n3\n\n\n25\n\n\n25\n\n\n16\n\n\nNot Expired\n\n\n\n\n\n\n4\n\n\n40\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n5\n\n\n21\n\n\n40\n\n\n31\n\n\nExpired\n\n\n\n\n\n\n6\n\n\n35\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n7\n\n\n45\n\n\n45\n\n\n36\n\n\nNot Expired\n\n\n\n\n\n\n8\n\n\n57\n\n\n57\n\n\n48\n\n\nNot Expired\n\n\n\n\n\n\n\n\nTime Buckets (A component of Managed State)\n\n\nOne of the requirements of the Deduper is to store all the unique\ntuples (actually, just the keys of tuples). Keeping an ever growing\ncache in memory is not scalable. So what we need is a limited cache\nbacked by a persistent store. When data is requested to be fetched from managed\nstate, it is also cached in an in-memory cache. Buckets help\nnarrow down the search of duplicates for incoming tuples. A Bucket is an\nabstraction for a collection of tuples all of which share a common hash\nvalue based on some hash function or a range of time, for example: a\nbucket of data for 5 contiguous minutes. A Bucket\u00a0has a span property called Bucket Span.\n\n\nBucket Span\n\n\nBucket span is simply the range of the domain\nthat is covered by the Bucket. This span is specified in\nthe domain of the Expiry key. If the Expiry\nKey is time, \u00a0then the Bucket span\nwill be specified in seconds. It is\nonly defined in case tuples have an Expiry Key.\n\n\nNumber of Buckets\n\n\nThe number of buckets can be given by - \nNum Buckets = Expiry\nPeriod / Bucket Span\n\n\nThis is because at any point of time, we need only store Expiry\nPeriod worth of data.\n\n\nExample - Buckets\n\n\n\n\nAssumptions\n\n\nAssumption 1 \n\n\nThis assumption is only applicable in case of Dedup with\nExpiry.\n\n\nFor any two tuples, t1 and t2 having dedup keys d1 and d2, and\nexpiry keys e1 and e2, respectively, the following holds:\n\n\nIf d1 = d2,\n  then e1 = e2\n\n\n\n\nIn other words, there may never\nbe\u00a0two tuples t1 and t2 such that:\n\n\nTuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2\n\n\n\n\nIn other words, any two tuples with the same dedup key are assumed to have the\nsame expiry key as well.\nThis assumption was made with respect to certain use cases. These\nuse cases follow this assumption in that the records which are\nduplicates are exactly identical. An example use case is when log\nmessages are replayed erroneously, and we want to identify the duplicate\nlog messages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.\n\n\nIn case the duplicate tuple has a different value for expiry key, the behavior of\nthe deduper can be non-deterministic.\n\n\nFlow of a Tuple through Dedup Operator\n\n\nTuples flow through the Dedup operator one by one. Deduper may process a tuple immediately, or store it in some data\nstructure for later processing.\n\n\nWhen a tuple always arrives at the input\nport\u00a0of the Dedup operator, it does\nthe following tasks.\n\n\nCheck if tuple is Expired\n\n\nThis is only done in case of Dedup with expiry. The\nfollowing condition is used to check if the tuple is expired.\n\n\nif ( Latest Point - Expiry Key \n Expiry Point )\n  then Expired\n\n\n\n\nIf the tuple is expired, then send it to the expired port.\n\n\nCheck if tuple is a Duplicate or Unique\n\n\nOnce a tuple passes the check of expiry, we proceed to check if\nthe tuple is a duplicate of some earlier tuple. Note that\nif the tuple in question is not expired, the duplicate will also not\nhave expired due to the assumption listed \nhere\n.\nThe Deduper queries the Managed state to fetch the value for the tuple key.\nThis request is processed by the Managed state in a separate asynchronous thread.\nOnce this request is submitted, the Deduper moves on to process other\ntuples. Additionally the Deduper also inserts the tuple being processed\ninto a waiting events\u00a0queue for later processing.\n\n\nProcess pending tuples\n\n\nOnce the Deduper has looked at the all the tuples in the current window,\nit starts to process the tuples in the waiting queue to finalize the decision\n(unique or duplicate) for these tuples.\nOnce the request to Managed state is completed for a tuple and the value is\nfetched from persistent storage, the Deduper can decide if the tuple in\nquestion is a duplicate or a unique.\nDepending on whether there is enough time left in the current window,\nit can do one of the following:\n\n\n\n\nProcess only the tuples for which the managed state has completed processing.\nThe tuples which are still being processed by managed state are skipped only to come back to them when it can no longer postpone it. This is typically done when the operator\nhas idle time as there are no tuples on the input ports and the current window\nhas still not ended.\n\n\nBlock on them to complete their processing. This will happen when the current\nwindow has no time left, and the decision cannot be postponed. Note: An operator can end its window, only when all the tuples have been completely processed.  \n\n\n\n\nPorts, Attributes and Properties\n\n\nPorts\n\n\nThe deduper has a single input port and multiple output\nports.\n\n\n\n\ninput\n - This is the input port through\n    which the tuples arrive at the Deduper.\n\n\nunique\n\u00a0- This is the output port on\n    which unique tuples are sent out by the Deduper.\n\n\nduplicate\n\u00a0- This is the output port on\n    which duplicate tuples are sent out by the Deduper.\n\n\nexpired\n\u00a0- This is the output port on\n    which expired tuples are sent out by the Deduper.\n\n\n\n\nThe user can choose which output ports to connect the down stream operators.\nAll the output ports are optional and can be used as required by the use case.\n\n\nAttributes\n\n\n\n\nInput port Attribute - input.TUPLE_CLASS\n\u00a0- Class or the fully\nqualified class name.\n\n\nMandatory attribute\n\n\nTells the operator about the type of the incoming\ntuple.\n\n\n\n\n\n\n\n\nProperties\n\n\n\n\n\n\nkeyExpression\n\u00a0- String\n\n\n\n\nMandatory parameter.\n\n\nThe java expression to extract the key fields in the incoming tuple (POJO)\n\n\n\n\n\n\n\n\ntimeExpression\n\u00a0- String - (Time Based Deduper only)\n\n\n\n\nThe java expression to extract the time field in the incoming tuple (POJO).\n\n\n\n\n\n\n\n\nexpireBefore\n\u00a0- Long (Seconds) - (Time Based Deduper only)\n\n\n\n\nThis is the total time period during which a tuple stays in the system and blocks any other tuple with the same key.\n\n\n\n\n\n\n\n\nbucketSpan\n\u00a0- Long (Seconds) - (Time Based Deduper only)\n\n\n\n\nMandatory parameter\n\n\nThis is the unit which describes how large a bucket can be. Typically this should be defined depending on the use case. For example, if we have expireBefore set to 1 hour, then typically we would be clubbing data in the order of minutes, so a \nbucketSpan\n of a few minutes would make sense. Note that in this case, the entire data worth the \nbucketSpan\n will expire as a whole. Setting it to 1 minute would make the number of time buckets in the system to be 1 hour / 1 minute = 60 buckets.  Similarly setting bucketSpan to 5 minutes would make number of buckets to be 12.\n\n\nNote that having too many or too few buckets could have a performance impact. If unsure, set the bucketSpan to the square root of \nexpireBefore\n. This way the number of buckets and bucket span are balanced.\n\n\n\n\n\n\n\n\nreferenceInstant\n\u00a0- \u00a0Long (Seconds) - (Time Based Deduper only)\n\n\n\n\nThe reference point from which to start the time which is use for expiry. Setting the referenceInstant to say, r seconds from the epoch, would initialize the start of expiry to be from that \ninstant = r\n. The start and end of the expiry window periodically move by the span of a single bucket.\n\n\n\n\n\n\n\n\nnumBuckets\n\u00a0- \u00a0Integer - (Bounded Deduper only)\n\n\n\n\nOptional parameter, but recommended to be provided by the user.\n\n\nThis is the number of buckets that need to be used for storing the keys of the incoming tuples.\n\n\nUsers can decide upon the proper value for this parameter by guessing the number of distinct keys in the application. A reasonable value is the square root of N, where N is the number of distinct keys. If omitted, the Java MAX_VALUE for integer is used for N.\n\n\n\n\n\n\n\n\nExample\n\n\nPlease refer to \nhttps://github.com/DataTorrent/examples/tree/master/tutorials/dedup\n\u00a0for\nan example on how to use Deduper.\n\n\nPartitioning\n\n\nDeduper can be statically partitioned using the operator\nattribute: PARTITIONER\n\n\nAdd the following property to the properties.xml file:\n\n\nproperty\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nname\ndt.operator.{OperatorName}.attr.PARTITIONER\n/name\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:2\n/value\n\n\n/property\n\n\n\n\n\nThis will partition the Dedup operator into 2 static partitions. Change the number\nto the required number of partitions.\n\n\nDynamic partitioning is currently not supported in the Deduper.", 
            "title": "Deduper"
        }, 
        {
            "location": "/operators/deduper/#deduper-operator-documentation", 
            "text": "", 
            "title": "Deduper - Operator Documentation"
        }, 
        {
            "location": "/operators/deduper/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/operators/deduper/#about-this-document", 
            "text": "This document is intended as a guide for understanding and using\nthe Dedup operator.", 
            "title": "About this document"
        }, 
        {
            "location": "/operators/deduper/#terminology", 
            "text": "We will refer to this operator as the Deduper or Dedup operator\ninterchangeably.", 
            "title": "Terminology"
        }, 
        {
            "location": "/operators/deduper/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/operators/deduper/#dedup-what-in-a-nutshell", 
            "text": "Dedup is actually short for Deduplication. Duplicates are omnipresent and\ncan be found in almost any kind of data. Most of the times it is\nessential to discard, or at the very least separate out the data into\nunique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one of which contains all unique tuples, and the other which are\noccurring more than once in the original data set.", 
            "title": "Dedup - \u201cWhat\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#dedup-how-in-a-nutshell", 
            "text": "In order to quickly decide whether an incoming tuple is duplicate\nor unique, it has to store each incoming tuple (or a signature, like key,\nfor example) to be used for comparison later. A plain in-memory storage\nmay work for small datasets, but will not scale for large ones. Deduper employs a large scale distributed persistent hashing mechanism (known as the Managed State) which allows\nit to identify if a particular tuple is duplicate or unique. Managed state is a layer on HDFS which allows all the stored data to be persisted in a distributed fashion.\nEach time it identifies a tuple as a unique tuple, it also\nstores it into the Managed state for future\nlookup.", 
            "title": "Dedup - \u201cHow\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#use-cases-basic-dedup", 
            "text": "", 
            "title": "Use cases - Basic Dedup"
        }, 
        {
            "location": "/operators/deduper/#dedup-key", 
            "text": "A dedup key is a set of one or more fields in the data tuple which\nacts as the key\u00a0for the tuples.\nThis is used by the deduper to compare tuples to arrive at the\nconclusion on whether two tuples are duplicates.  Consider an example schema and two sample tuples  {Name, Phone, Email, Date, State, Zip, Country}  Tuple 1:  {\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}  Tuple 2:  {\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}  Let us assume that the Dedup Key\nis  {Name, Phone} . In\nthis case, the two tuples are duplicates because the key fields are same\nin both the tuples. However, if the Dedup Key is {Phone,Email},\nthe two are unique as the email values differ.", 
            "title": "Dedup Key"
        }, 
        {
            "location": "/operators/deduper/#use-case-details", 
            "text": "Consider the case of de-duplicating a master data set\nwhich is stored in a file.\u00a0Further also consider the\nfollowing schema for tuples in the data set.  {Name, Phone, Email, Date, City, Zip, Country}  Also consider that we need to identify unique customers from the\nmaster data set. So, ultimately the output needed for the use case is\ntwo data sets - Unique Records\u00a0and Duplicate Records.  As part of configuring the operator for this use case, we need to\nset the following parameters:   keyExpression \u00a0- This can be set as\n    the primary key which can be used to uniquely identify a Customer.\n    For example, we can set it to  Name,Email   The above configuration is sufficient to address this use case.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-case-dedup-with-expiry", 
            "text": "", 
            "title": "Use case - Dedup with Expiry"
        }, 
        {
            "location": "/operators/deduper/#motivation", 
            "text": "The Basic Dedup use case is the most straightforward and is\nusually applied when the amount of data to be processed is not huge.\nHowever, if the incoming data is huge, or even never-ending, it is\nusually not necessary to keep storing all the data. This is because in\nmost real world use cases, the duplicates occur only a short distance\napart. Hence, after a while, it is usually okay to forget part of\nthe history and consider only limited history for identifying\nduplicates, in the interest of efficiency. In other words, we expire\n(ignore) some tuples which are (or were supposed to be) delivered long\nback. Doing so, reduces the load on the storage mechanism (managed state) which effectively deletes part of the history, thus making the whole process more\nefficient. We call this use case, Dedup with expiry.", 
            "title": "Motivation"
        }, 
        {
            "location": "/operators/deduper/#expiry-key", 
            "text": "The easiest way to understand this use case is to consider\ntime\u00a0as the criterion for expiring\ntuples. Time\u00a0is a natural expiry\nkey and is in line with the concept of expiry. Formally, an expiry field\nis a field in the input tuple which can be used to discard incoming\ntuples as expired. This expiry key\nusually works with another parameter called Expiry Period defined\nnext.", 
            "title": "Expiry Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-period", 
            "text": "The expiry period is the value supplied by the user to define the\nextent of history which should be considered while expiring\ntuples.", 
            "title": "Expiry Period"
        }, 
        {
            "location": "/operators/deduper/#use-case-details_1", 
            "text": "Consider an incoming stream of system logs. The use case requires\nus to identify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is 30-12-2014 00:00:00  and the\nlatest message that the system has encountered had the time stamp 31-12-2014 00:00:00 , then the\nincoming message must be considered as expired. However, if the incoming\nmessage had any timestamp like  30-12-2014\n00:11:00 , it must be accepted into the system and be checked for a possible duplicate.  The expiry facet in the use case above gives us an advantage in\nthat we do not have to compare the incoming record with all\u00a0the data to check if it is a duplicate.\nAt the same time, all\u00a0the\nincoming data need not be stored; just a day worth of data is adequate to address the above use case.  Configuring the below parameters will solve the problem for this\nuse case:   keyExpression \u00a0- This is the dedup key for the incoming tuples (similar to the Basic Dedup use case). This can be any key which can uniquely identify a record. For log messages this can be a serial number attached in the log.  timeExpression \u00a0- This is the key which can help identify the expired records, as explained above. In this particular use case, it can be a timestamp field which indicates when the log message was generated.  expireBefore \u00a0- This is the period of expiry as explained above. In our example use case this will be 24 hour, specified in seconds.   Configuration of the above parameters is sufficient to address this use\ncase.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-cases-summary", 
            "text": "Basic Dedup  - Deduplication of\n    bounded datasets. Data is assumed to be bounded. This use case is\n    not meant for never ending streams of data. For example:\n    Deduplication of master data like customer records, product catalogs\n    etc.  Time Based Dedup \u00a0- Deduplication of\n    unlimited streams of data. This use case handles unbounded streams\n    of data and can run forever. An expiry key and criterion is expected\n    as part of the input which helps avoid storing all the unique data.\n    This helps speed up performance. Any timestamp field in the incoming\n    tuple can be used as a time based expiry key.  With respect to system time \u00a0- Time progresses with system time. Any expiry criteria are executed with the notion of system time. This is possible if the incoming tuple does not have a time field, or the user does not specify a  timeExpression .  With respect to tuple time \u00a0- Time progresses based on the time in the incoming tuples. Expiry criteria are executed with the notion of time indicated by the incoming tuple. Specification of the time field ( timeExpression ) is mandatory for this scenario.", 
            "title": "Use cases - Summary"
        }, 
        {
            "location": "/operators/deduper/#technical-architecture", 
            "text": "", 
            "title": "Technical Architecture"
        }, 
        {
            "location": "/operators/deduper/#class-structure", 
            "text": "", 
            "title": "Class Structure"
        }, 
        {
            "location": "/operators/deduper/#architectural-details", 
            "text": "", 
            "title": "Architectural Details"
        }, 
        {
            "location": "/operators/deduper/#concepts", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/operators/deduper/#dedup-key-specified-by-keyexpression-parameter", 
            "text": "A dedup key is a set of one or more fields in the data tuple which\nacts as the key\u00a0for the tuples.\nThis is used by the deduper to compare tuples to arrive at the\nconclusion on whether two tuples are duplicates. If Dedup Key of two\ntuples match, then they are duplicates, else they are unique.", 
            "title": "Dedup Key - Specified by keyExpression\u00a0parameter"
        }, 
        {
            "location": "/operators/deduper/#expiry-key-specified-by-timeexpression-parameter", 
            "text": "A tuple may or may not have an Expiry Key. Dedup operator cannot\nkeep storing all the data that is flowing into the operator. At some\npoint it becomes essential to discard some of the historical tuples in\ninterest of memory and efficiency.  At the same time, tuples are expected to arrive at the Dedup\noperator within some time after they are generated. After this time, the\ntuples may be considered as stale or obsolete.  In such cases, the Deduper considers these tuples as\nexpired\u00a0and takes no action other than\nseparating out these tuples on a different port in order to be processed\nby some other operator or stored offline for analysis.  In order to create a criterion for discarding such tuples, we\nintroduce an Expiry Key. Looking at the value of the Expiry Key in each\ntuple, we can decide whether or not to discard this tuple as\nexpired.  The expiry key that we consider in Time Based Dedup is\ntime. This usually works with\nanother parameter called Expiry Period defined next.", 
            "title": "Expiry Key - Specified by timeExpression\u00a0parameter"
        }, 
        {
            "location": "/operators/deduper/#expiry-period_1", 
            "text": "The Expiry Period is the value supplied by the user which decides\nwhen a particular tuple expires.", 
            "title": "Expiry Period"
        }, 
        {
            "location": "/operators/deduper/#time-points", 
            "text": "For every dataset that the deduper processes, a set of time points is maintained:   Latest Point \u00a0- This is the maximum\n    time point observed in all the processed tuples.  Expiry Point \u00a0- This is given by:\n     Expiry Point = Latest Point - Expiry Period   These points help the deduper to make decisions related to expiry\nof a tuple.", 
            "title": "Time Points"
        }, 
        {
            "location": "/operators/deduper/#example-expiry", 
            "text": "Tuple Id  Expiry Key (Expiry Period = 10)  Latest Point  Expiry Point  Decision for Tuple      1  10  10  1  Not Expired    2  20  20  11  Not Expired    3  25  25  16  Not Expired    4  40  40  31  Not Expired    5  21  40  31  Expired    6  35  40  31  Not Expired    7  45  45  36  Not Expired    8  57  57  48  Not Expired", 
            "title": "Example - Expiry"
        }, 
        {
            "location": "/operators/deduper/#time-buckets-a-component-of-managed-state", 
            "text": "One of the requirements of the Deduper is to store all the unique\ntuples (actually, just the keys of tuples). Keeping an ever growing\ncache in memory is not scalable. So what we need is a limited cache\nbacked by a persistent store. When data is requested to be fetched from managed\nstate, it is also cached in an in-memory cache. Buckets help\nnarrow down the search of duplicates for incoming tuples. A Bucket is an\nabstraction for a collection of tuples all of which share a common hash\nvalue based on some hash function or a range of time, for example: a\nbucket of data for 5 contiguous minutes. A Bucket\u00a0has a span property called Bucket Span.", 
            "title": "Time Buckets (A component of Managed State)"
        }, 
        {
            "location": "/operators/deduper/#bucket-span", 
            "text": "Bucket span is simply the range of the domain\nthat is covered by the Bucket. This span is specified in\nthe domain of the Expiry key. If the Expiry\nKey is time, \u00a0then the Bucket span\nwill be specified in seconds. It is\nonly defined in case tuples have an Expiry Key.", 
            "title": "Bucket Span"
        }, 
        {
            "location": "/operators/deduper/#number-of-buckets", 
            "text": "The number of buckets can be given by -  Num Buckets = Expiry\nPeriod / Bucket Span  This is because at any point of time, we need only store Expiry\nPeriod worth of data.", 
            "title": "Number of Buckets"
        }, 
        {
            "location": "/operators/deduper/#example-buckets", 
            "text": "", 
            "title": "Example - Buckets"
        }, 
        {
            "location": "/operators/deduper/#assumptions", 
            "text": "", 
            "title": "Assumptions"
        }, 
        {
            "location": "/operators/deduper/#assumption-1", 
            "text": "This assumption is only applicable in case of Dedup with\nExpiry.  For any two tuples, t1 and t2 having dedup keys d1 and d2, and\nexpiry keys e1 and e2, respectively, the following holds:  If d1 = d2,\n  then e1 = e2  In other words, there may never\nbe\u00a0two tuples t1 and t2 such that:  Tuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2  In other words, any two tuples with the same dedup key are assumed to have the\nsame expiry key as well.\nThis assumption was made with respect to certain use cases. These\nuse cases follow this assumption in that the records which are\nduplicates are exactly identical. An example use case is when log\nmessages are replayed erroneously, and we want to identify the duplicate\nlog messages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.  In case the duplicate tuple has a different value for expiry key, the behavior of\nthe deduper can be non-deterministic.", 
            "title": "Assumption 1 "
        }, 
        {
            "location": "/operators/deduper/#flow-of-a-tuple-through-dedup-operator", 
            "text": "Tuples flow through the Dedup operator one by one. Deduper may process a tuple immediately, or store it in some data\nstructure for later processing.  When a tuple always arrives at the input\nport\u00a0of the Dedup operator, it does\nthe following tasks.", 
            "title": "Flow of a Tuple through Dedup Operator"
        }, 
        {
            "location": "/operators/deduper/#check-if-tuple-is-expired", 
            "text": "This is only done in case of Dedup with expiry. The\nfollowing condition is used to check if the tuple is expired.  if ( Latest Point - Expiry Key   Expiry Point )\n  then Expired  If the tuple is expired, then send it to the expired port.", 
            "title": "Check if tuple is Expired"
        }, 
        {
            "location": "/operators/deduper/#check-if-tuple-is-a-duplicate-or-unique", 
            "text": "Once a tuple passes the check of expiry, we proceed to check if\nthe tuple is a duplicate of some earlier tuple. Note that\nif the tuple in question is not expired, the duplicate will also not\nhave expired due to the assumption listed  here .\nThe Deduper queries the Managed state to fetch the value for the tuple key.\nThis request is processed by the Managed state in a separate asynchronous thread.\nOnce this request is submitted, the Deduper moves on to process other\ntuples. Additionally the Deduper also inserts the tuple being processed\ninto a waiting events\u00a0queue for later processing.", 
            "title": "Check if tuple is a Duplicate or Unique"
        }, 
        {
            "location": "/operators/deduper/#process-pending-tuples", 
            "text": "Once the Deduper has looked at the all the tuples in the current window,\nit starts to process the tuples in the waiting queue to finalize the decision\n(unique or duplicate) for these tuples.\nOnce the request to Managed state is completed for a tuple and the value is\nfetched from persistent storage, the Deduper can decide if the tuple in\nquestion is a duplicate or a unique.\nDepending on whether there is enough time left in the current window,\nit can do one of the following:   Process only the tuples for which the managed state has completed processing.\nThe tuples which are still being processed by managed state are skipped only to come back to them when it can no longer postpone it. This is typically done when the operator\nhas idle time as there are no tuples on the input ports and the current window\nhas still not ended.  Block on them to complete their processing. This will happen when the current\nwindow has no time left, and the decision cannot be postponed. Note: An operator can end its window, only when all the tuples have been completely processed.", 
            "title": "Process pending tuples"
        }, 
        {
            "location": "/operators/deduper/#ports-attributes-and-properties", 
            "text": "", 
            "title": "Ports, Attributes and Properties"
        }, 
        {
            "location": "/operators/deduper/#ports", 
            "text": "The deduper has a single input port and multiple output\nports.   input  - This is the input port through\n    which the tuples arrive at the Deduper.  unique \u00a0- This is the output port on\n    which unique tuples are sent out by the Deduper.  duplicate \u00a0- This is the output port on\n    which duplicate tuples are sent out by the Deduper.  expired \u00a0- This is the output port on\n    which expired tuples are sent out by the Deduper.   The user can choose which output ports to connect the down stream operators.\nAll the output ports are optional and can be used as required by the use case.", 
            "title": "Ports"
        }, 
        {
            "location": "/operators/deduper/#attributes", 
            "text": "Input port Attribute - input.TUPLE_CLASS \u00a0- Class or the fully\nqualified class name.  Mandatory attribute  Tells the operator about the type of the incoming\ntuple.", 
            "title": "Attributes"
        }, 
        {
            "location": "/operators/deduper/#properties", 
            "text": "keyExpression \u00a0- String   Mandatory parameter.  The java expression to extract the key fields in the incoming tuple (POJO)     timeExpression \u00a0- String - (Time Based Deduper only)   The java expression to extract the time field in the incoming tuple (POJO).     expireBefore \u00a0- Long (Seconds) - (Time Based Deduper only)   This is the total time period during which a tuple stays in the system and blocks any other tuple with the same key.     bucketSpan \u00a0- Long (Seconds) - (Time Based Deduper only)   Mandatory parameter  This is the unit which describes how large a bucket can be. Typically this should be defined depending on the use case. For example, if we have expireBefore set to 1 hour, then typically we would be clubbing data in the order of minutes, so a  bucketSpan  of a few minutes would make sense. Note that in this case, the entire data worth the  bucketSpan  will expire as a whole. Setting it to 1 minute would make the number of time buckets in the system to be 1 hour / 1 minute = 60 buckets.  Similarly setting bucketSpan to 5 minutes would make number of buckets to be 12.  Note that having too many or too few buckets could have a performance impact. If unsure, set the bucketSpan to the square root of  expireBefore . This way the number of buckets and bucket span are balanced.     referenceInstant \u00a0- \u00a0Long (Seconds) - (Time Based Deduper only)   The reference point from which to start the time which is use for expiry. Setting the referenceInstant to say, r seconds from the epoch, would initialize the start of expiry to be from that  instant = r . The start and end of the expiry window periodically move by the span of a single bucket.     numBuckets \u00a0- \u00a0Integer - (Bounded Deduper only)   Optional parameter, but recommended to be provided by the user.  This is the number of buckets that need to be used for storing the keys of the incoming tuples.  Users can decide upon the proper value for this parameter by guessing the number of distinct keys in the application. A reasonable value is the square root of N, where N is the number of distinct keys. If omitted, the Java MAX_VALUE for integer is used for N.", 
            "title": "Properties"
        }, 
        {
            "location": "/operators/deduper/#example", 
            "text": "Please refer to  https://github.com/DataTorrent/examples/tree/master/tutorials/dedup \u00a0for\nan example on how to use Deduper.", 
            "title": "Example"
        }, 
        {
            "location": "/operators/deduper/#partitioning", 
            "text": "Deduper can be statically partitioned using the operator\nattribute: PARTITIONER  Add the following property to the properties.xml file:  property \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 name dt.operator.{OperatorName}.attr.PARTITIONER /name \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 value com.datatorrent.common.partitioner.StatelessPartitioner:2 /value  /property   This will partition the Dedup operator into 2 static partitions. Change the number\nto the required number of partitions.  Dynamic partitioning is currently not supported in the Deduper.", 
            "title": "Partitioning"
        }
    ]
}